# Map-to-reference Workflow

## Context

As part of the amplicon pool sequencing work carried out by the sequencing and systematics platform at the National Museum of Natural History (MNHN), two bioinformatics workflows have been developed to automate the processing of data from Illumina sequencing.

These workflows aim to standardise the analysis steps, limit manual manipulation and simplify the execution of bioinformatic processing, from raw sequencing files to the final usable sequences. They are deployed and executable on the Galaxy platform.

Two complementary approaches are proposed:
- A Map-to-reference workflow, intended for datasets with a reference sequence
- A De novo workflow, used when no reference sequence is available
This repository describes the Map-to-reference workflow.

## General description of the workflow

The Map-to-reference workflow allows Illumina sequencing data from amplicon pools to be processed using a known reference sequence. It automates all stages of analysis, from raw FASTQ files to the generation of a consensus sequence and an associated metadata file.

## Input data

The workflow requires the following:
- FASTQ files of forward (R1) and reverse (R2) reads
- Reference sequence in FASTA format
- Forward and reverse primer sequences (text format)
- Minimum and maximum read length (optional, user-defined)

## Workflow steps

### Removal of contaminants or known sequences

This step aims to remove from the database any reads that map to a given reference sequence, corresponding, for example, to contaminants or a known taxon sharing the same primers as other sequences of interest.

The FASTQ R1 and R2 files are first grouped into a compressed collection. The reads are then mapped against the reference sequence using Bowtie2. A second mapping is performed considering inverted reads to include all possible orientations.

The mapping parameters are deliberately strict to minimise information loss and avoid the erroneous elimination of reads. Unmapped reads are finally retrieved in the form of two separate files (sense and antisense), which can be used in subsequent analyses, particularly with the De novo workflow.

### Read pairing and filtering

Paired reads are assembled using the PEAR tool.

The sequences obtained are then filtered by length using Filter FASTQ, according to the threshold defined by the user when launching the workflow.

### Quality control

Quality control is performed at various stages of processing using FastQC:
- On raw reads (R1 and R2) before pairing
- On reads after pairing and filtering
This step allows us to check the evolution of data quality throughout the pipeline.

### Conversion of primers to FASTA format

The primer sequences are initially provided in text format. They are converted to FASTA format using a series of Galaxy tools in order to produce a format compatible with the subsequent stages of the workflow. These sequences will be used in particular to remove the primers from the final consensus sequence.

### Alignment to the reference sequence and generation of the consensus sequence

Paired reads are mapped to the reference sequence using Bowtie2.

The result is a BAM file containing all aligned reads, accompanied by their alignment quality score (MAPQ).

Filtering is then performed with Samtools view to retain only reads with an alignment quality greater than 30. The alignments can be viewed along the reference sequence using JBrowse2.

The filtered reads are then converted to FASTA format and used to generate a consensus sequence using MEGAHIT. Finally, the primer sequences are removed from the consensus sequence using Cutadapt and primers previously converted to FASTA format.

## Output files

The workflow produces the following:
- Final consensus sequence (without primers)
- Quality control files (FastQC)
- Metadata file

## Metadata file

A metadata file is automatically generated by the workflow. It contains, in particular:
- The total number of reads mapped to the reference with a MAPQ greater than 30
- The length of the consensus sequence
- The minimum, maximum and average coverage depth
The creation of this file involves several steps of selection, renaming and normalisation of columns from intermediate tools to provide a consistent and easily usable format.
