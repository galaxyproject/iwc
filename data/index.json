[
    {
        "name": "hic-fastq-to-cool-hicup-cooler",
        "trsID": "#workflow/github.com/iwc-workflows/hic-hicup-cooler/hic-fastq-to-cool-hicup-cooler",
        "iwcID": "hic-hicup-cooler-hic-fastq-to-cool-hicup-cooler",
        "readme": "# (Capture) Hi-C Processing: FASTQ to Balanced Cool Files\n\nThis can also be used for Hi-ChIP experiments, in that case the output with `matrix with iced values` is ignored and the matrix to use is `matrix with raw values`.\n\n## Input datasets\n\n- The workflow needs a list of dataset pairs of fastqsanger.\n\n## Input values\n\n- genome name: suggested from the bowtie2 indices, it is used to map and build the list of bins.\n- restriction enzyme: Restriction enzyme used e.g. A^GATCT,BglII. The '^' is used to express where the enzyme cuts.\n- No fill-in: If you used a biotin fill-in protocol, put this to false, else, put it to true.\n- minimum MAPQ: Filtering to apply to pairs you want to keep in your matrix, set it to 0 to not apply filtering (HiCUP already filter for uniquely mapped or MAPQ30).\n- Bin size in bp: Used to generate your first matrix but you will be able to rerun the subworkflow `hic_tabix_to_cool_cooler` to get other resolutions.\n- Interactions to consider to calculate weights in normalization step: this is a parameter for the last correction step (ICE).\n\nFor the region capture workflow:\n\n- chromosome, start and end positions of the capture region\n\nFor the Hi-C workflow:\n\n- region to use in pyGenomeTracks to check the matrices.\n\n## Processing\n\n- Reads are processed with HiCUP which comprises these steps:\n  - Truncation of reads for the religation motif\n  - Mapping of mates independently with bowtie2\n  - Pairing the mates when both mates are uniquely mapped or MAPQ30\n  - Filtering the pairs for undigested, self-ligated...\n  - Removing duplicates\n- The output BAM file is converted to medium juicer format: `<readname> <str1> <chr1> <pos1> <frag1> <str2> <chr2> <pos2> <frag2> <mapq1> <mapq2>` where str = strand (0 for forward, anything else for reverse) and pos is the middle of the fragment.\n- The pairs are filtered for MAPQ if specified.\n- For the region capture Hi-C workflow the pairs are filtered for both mates in the captured region.\n- The filtered pairs are sorted and indexed with cooler_csort.\n- The pairs are loaded into a matrix of the given resolution and balanced with cooler.\n- A final plot is made with pyGenomeTracks using the balanced matrices on the region provided or the capture region.\n\n## Subworkflows\n\nThere are 2 subworkflows: `hic_tabix_to_cool_cooler` and `hic_fastq_to_pairs_hicup.ga`.\n\n### hic_tabix_to_cool_cooler\n\nThis first subworkflow can be used to generate matrices to different resolutions using one of the output of the full workflow (`valid pairs filtered and sorted`).\n\nIf the dataset are still in galaxy (format: juicer_medium_tabix.gz), the workflow can be run directly.\n\nIf the dataset is not anymore in galaxy, you need to upload and specify the datatype as: juicer_medium_tabix.gz\n\n### hic_fastq_to_pairs_hicup\n\nThe second subworkflow has no real reason to be launched by itself except for QC tests.\n\nIf you want to run the first subworkflow from these results:\n\n- You first need to filter the pairs (`valid pairs in juicebox format MAPQ filtered`) for the capture region if relevent using the tool Filter1 (**Filter** data on any column using simple expressions) with the condition `(c3=='chr2' and c4<180000000 and c4>170000000) and (c7==\"chr2\" and c8<180000000 and c8>170000000)` if your capture region is chr2:170000000-180000000.\n- Then you need to run cooler_csort (**cooler csort with tabix** Sort and index a contact list.) with as input the `valid pairs in juicebox format MAPQ filtered` or the output of the previous step and for \"Format of your input file\" use \"Juicer Medium Format\".\n\nThe output of `cooler_csort` can be used as input of the first subworkflow.\n",
        "updated": "2023-09-08T00:00:00",
        "categories": [],
        "collections": [
            "Epigenetics"
        ]
    },
    {
        "name": "chic-fastq-to-cool-hicup-cooler",
        "trsID": "#workflow/github.com/iwc-workflows/hic-hicup-cooler/chic-fastq-to-cool-hicup-cooler",
        "iwcID": "hic-hicup-cooler-chic-fastq-to-cool-hicup-cooler",
        "readme": "# (Capture) Hi-C Processing: FASTQ to Balanced Cool Files\n\nThis can also be used for Hi-ChIP experiments, in that case the output with `matrix with iced values` is ignored and the matrix to use is `matrix with raw values`.\n\n## Input datasets\n\n- The workflow needs a list of dataset pairs of fastqsanger.\n\n## Input values\n\n- genome name: suggested from the bowtie2 indices, it is used to map and build the list of bins.\n- restriction enzyme: Restriction enzyme used e.g. A^GATCT,BglII. The '^' is used to express where the enzyme cuts.\n- No fill-in: If you used a biotin fill-in protocol, put this to false, else, put it to true.\n- minimum MAPQ: Filtering to apply to pairs you want to keep in your matrix, set it to 0 to not apply filtering (HiCUP already filter for uniquely mapped or MAPQ30).\n- Bin size in bp: Used to generate your first matrix but you will be able to rerun the subworkflow `hic_tabix_to_cool_cooler` to get other resolutions.\n- Interactions to consider to calculate weights in normalization step: this is a parameter for the last correction step (ICE).\n\nFor the region capture workflow:\n\n- chromosome, start and end positions of the capture region\n\nFor the Hi-C workflow:\n\n- region to use in pyGenomeTracks to check the matrices.\n\n## Processing\n\n- Reads are processed with HiCUP which comprises these steps:\n  - Truncation of reads for the religation motif\n  - Mapping of mates independently with bowtie2\n  - Pairing the mates when both mates are uniquely mapped or MAPQ30\n  - Filtering the pairs for undigested, self-ligated...\n  - Removing duplicates\n- The output BAM file is converted to medium juicer format: `<readname> <str1> <chr1> <pos1> <frag1> <str2> <chr2> <pos2> <frag2> <mapq1> <mapq2>` where str = strand (0 for forward, anything else for reverse) and pos is the middle of the fragment.\n- The pairs are filtered for MAPQ if specified.\n- For the region capture Hi-C workflow the pairs are filtered for both mates in the captured region.\n- The filtered pairs are sorted and indexed with cooler_csort.\n- The pairs are loaded into a matrix of the given resolution and balanced with cooler.\n- A final plot is made with pyGenomeTracks using the balanced matrices on the region provided or the capture region.\n\n## Subworkflows\n\nThere are 2 subworkflows: `hic_tabix_to_cool_cooler` and `hic_fastq_to_pairs_hicup.ga`.\n\n### hic_tabix_to_cool_cooler\n\nThis first subworkflow can be used to generate matrices to different resolutions using one of the output of the full workflow (`valid pairs filtered and sorted`).\n\nIf the dataset are still in galaxy (format: juicer_medium_tabix.gz), the workflow can be run directly.\n\nIf the dataset is not anymore in galaxy, you need to upload and specify the datatype as: juicer_medium_tabix.gz\n\n### hic_fastq_to_pairs_hicup\n\nThe second subworkflow has no real reason to be launched by itself except for QC tests.\n\nIf you want to run the first subworkflow from these results:\n\n- You first need to filter the pairs (`valid pairs in juicebox format MAPQ filtered`) for the capture region if relevent using the tool Filter1 (**Filter** data on any column using simple expressions) with the condition `(c3=='chr2' and c4<180000000 and c4>170000000) and (c7==\"chr2\" and c8<180000000 and c8>170000000)` if your capture region is chr2:170000000-180000000.\n- Then you need to run cooler_csort (**cooler csort with tabix** Sort and index a contact list.) with as input the `valid pairs in juicebox format MAPQ filtered` or the output of the previous step and for \"Format of your input file\" use \"Juicer Medium Format\".\n\nThe output of `cooler_csort` can be used as input of the first subworkflow.\n",
        "updated": "2023-09-08T00:00:00",
        "categories": [],
        "collections": [
            "Epigenetics"
        ]
    },
    {
        "name": "hic-juicermediumtabix-to-cool-cooler",
        "trsID": "#workflow/github.com/iwc-workflows/hic-hicup-cooler/hic-juicermediumtabix-to-cool-cooler",
        "iwcID": "hic-hicup-cooler-hic-juicermediumtabix-to-cool-cooler",
        "readme": "# (Capture) Hi-C Processing: FASTQ to Balanced Cool Files\n\nThis can also be used for Hi-ChIP experiments, in that case the output with `matrix with iced values` is ignored and the matrix to use is `matrix with raw values`.\n\n## Input datasets\n\n- The workflow needs a list of dataset pairs of fastqsanger.\n\n## Input values\n\n- genome name: suggested from the bowtie2 indices, it is used to map and build the list of bins.\n- restriction enzyme: Restriction enzyme used e.g. A^GATCT,BglII. The '^' is used to express where the enzyme cuts.\n- No fill-in: If you used a biotin fill-in protocol, put this to false, else, put it to true.\n- minimum MAPQ: Filtering to apply to pairs you want to keep in your matrix, set it to 0 to not apply filtering (HiCUP already filter for uniquely mapped or MAPQ30).\n- Bin size in bp: Used to generate your first matrix but you will be able to rerun the subworkflow `hic_tabix_to_cool_cooler` to get other resolutions.\n- Interactions to consider to calculate weights in normalization step: this is a parameter for the last correction step (ICE).\n\nFor the region capture workflow:\n\n- chromosome, start and end positions of the capture region\n\nFor the Hi-C workflow:\n\n- region to use in pyGenomeTracks to check the matrices.\n\n## Processing\n\n- Reads are processed with HiCUP which comprises these steps:\n  - Truncation of reads for the religation motif\n  - Mapping of mates independently with bowtie2\n  - Pairing the mates when both mates are uniquely mapped or MAPQ30\n  - Filtering the pairs for undigested, self-ligated...\n  - Removing duplicates\n- The output BAM file is converted to medium juicer format: `<readname> <str1> <chr1> <pos1> <frag1> <str2> <chr2> <pos2> <frag2> <mapq1> <mapq2>` where str = strand (0 for forward, anything else for reverse) and pos is the middle of the fragment.\n- The pairs are filtered for MAPQ if specified.\n- For the region capture Hi-C workflow the pairs are filtered for both mates in the captured region.\n- The filtered pairs are sorted and indexed with cooler_csort.\n- The pairs are loaded into a matrix of the given resolution and balanced with cooler.\n- A final plot is made with pyGenomeTracks using the balanced matrices on the region provided or the capture region.\n\n## Subworkflows\n\nThere are 2 subworkflows: `hic_tabix_to_cool_cooler` and `hic_fastq_to_pairs_hicup.ga`.\n\n### hic_tabix_to_cool_cooler\n\nThis first subworkflow can be used to generate matrices to different resolutions using one of the output of the full workflow (`valid pairs filtered and sorted`).\n\nIf the dataset are still in galaxy (format: juicer_medium_tabix.gz), the workflow can be run directly.\n\nIf the dataset is not anymore in galaxy, you need to upload and specify the datatype as: juicer_medium_tabix.gz\n\n### hic_fastq_to_pairs_hicup\n\nThe second subworkflow has no real reason to be launched by itself except for QC tests.\n\nIf you want to run the first subworkflow from these results:\n\n- You first need to filter the pairs (`valid pairs in juicebox format MAPQ filtered`) for the capture region if relevent using the tool Filter1 (**Filter** data on any column using simple expressions) with the condition `(c3=='chr2' and c4<180000000 and c4>170000000) and (c7==\"chr2\" and c8<180000000 and c8>170000000)` if your capture region is chr2:170000000-180000000.\n- Then you need to run cooler_csort (**cooler csort with tabix** Sort and index a contact list.) with as input the `valid pairs in juicebox format MAPQ filtered` or the output of the previous step and for \"Format of your input file\" use \"Juicer Medium Format\".\n\nThe output of `cooler_csort` can be used as input of the first subworkflow.\n",
        "updated": "2023-09-08T00:00:00",
        "categories": [],
        "collections": [
            "Epigenetics"
        ]
    },
    {
        "name": "hic-fastq-to-pairs-hicup",
        "trsID": "#workflow/github.com/iwc-workflows/hic-hicup-cooler/hic-fastq-to-pairs-hicup",
        "iwcID": "hic-hicup-cooler-hic-fastq-to-pairs-hicup",
        "readme": "# (Capture) Hi-C Processing: FASTQ to Balanced Cool Files\n\nThis can also be used for Hi-ChIP experiments, in that case the output with `matrix with iced values` is ignored and the matrix to use is `matrix with raw values`.\n\n## Input datasets\n\n- The workflow needs a list of dataset pairs of fastqsanger.\n\n## Input values\n\n- genome name: suggested from the bowtie2 indices, it is used to map and build the list of bins.\n- restriction enzyme: Restriction enzyme used e.g. A^GATCT,BglII. The '^' is used to express where the enzyme cuts.\n- No fill-in: If you used a biotin fill-in protocol, put this to false, else, put it to true.\n- minimum MAPQ: Filtering to apply to pairs you want to keep in your matrix, set it to 0 to not apply filtering (HiCUP already filter for uniquely mapped or MAPQ30).\n- Bin size in bp: Used to generate your first matrix but you will be able to rerun the subworkflow `hic_tabix_to_cool_cooler` to get other resolutions.\n- Interactions to consider to calculate weights in normalization step: this is a parameter for the last correction step (ICE).\n\nFor the region capture workflow:\n\n- chromosome, start and end positions of the capture region\n\nFor the Hi-C workflow:\n\n- region to use in pyGenomeTracks to check the matrices.\n\n## Processing\n\n- Reads are processed with HiCUP which comprises these steps:\n  - Truncation of reads for the religation motif\n  - Mapping of mates independently with bowtie2\n  - Pairing the mates when both mates are uniquely mapped or MAPQ30\n  - Filtering the pairs for undigested, self-ligated...\n  - Removing duplicates\n- The output BAM file is converted to medium juicer format: `<readname> <str1> <chr1> <pos1> <frag1> <str2> <chr2> <pos2> <frag2> <mapq1> <mapq2>` where str = strand (0 for forward, anything else for reverse) and pos is the middle of the fragment.\n- The pairs are filtered for MAPQ if specified.\n- For the region capture Hi-C workflow the pairs are filtered for both mates in the captured region.\n- The filtered pairs are sorted and indexed with cooler_csort.\n- The pairs are loaded into a matrix of the given resolution and balanced with cooler.\n- A final plot is made with pyGenomeTracks using the balanced matrices on the region provided or the capture region.\n\n## Subworkflows\n\nThere are 2 subworkflows: `hic_tabix_to_cool_cooler` and `hic_fastq_to_pairs_hicup.ga`.\n\n### hic_tabix_to_cool_cooler\n\nThis first subworkflow can be used to generate matrices to different resolutions using one of the output of the full workflow (`valid pairs filtered and sorted`).\n\nIf the dataset are still in galaxy (format: juicer_medium_tabix.gz), the workflow can be run directly.\n\nIf the dataset is not anymore in galaxy, you need to upload and specify the datatype as: juicer_medium_tabix.gz\n\n### hic_fastq_to_pairs_hicup\n\nThe second subworkflow has no real reason to be launched by itself except for QC tests.\n\nIf you want to run the first subworkflow from these results:\n\n- You first need to filter the pairs (`valid pairs in juicebox format MAPQ filtered`) for the capture region if relevent using the tool Filter1 (**Filter** data on any column using simple expressions) with the condition `(c3=='chr2' and c4<180000000 and c4>170000000) and (c7==\"chr2\" and c8<180000000 and c8>170000000)` if your capture region is chr2:170000000-180000000.\n- Then you need to run cooler_csort (**cooler csort with tabix** Sort and index a contact list.) with as input the `valid pairs in juicebox format MAPQ filtered` or the output of the previous step and for \"Format of your input file\" use \"Juicer Medium Format\".\n\nThe output of `cooler_csort` can be used as input of the first subworkflow.\n",
        "updated": "2023-09-08T00:00:00",
        "categories": [],
        "collections": [
            "Epigenetics"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/atacseq/main",
        "iwcID": "atacseq-main",
        "readme": "# ATAC-seq Analysis: Chromatin Accessibility Profiling\n\nThis workflow is highly concordant with the corresponding training material.\nYou can have more information about ATAC-seq analysis in the [slides](https://training.galaxyproject.org/training-material/topics/epigenetics/tutorials/atac-seq/slides.html) and the [tutorial](https://training.galaxyproject.org/training-material/topics/epigenetics/tutorials/atac-seq/tutorial.html).\n\n## Inputs dataset\n\n- The workflow needs a single input which is a list of dataset pairs of fastqsanger.\n\n## Inputs values\n\n- reference_genome: this field will be adapted to the genomes available for bowtie2 and the genomes available for bedtools slopbed (dbkeys table)\n- effective_genome_size: this is used by macs2 and may be entered manually (indications are provided for heavily used genomes)\n- bin_size: this is used when normalization of coverage is performed. Large values will allow to have smaller output files but with less resolution while small values will increase computation time and size of output files to produce more resolutive bigwigs.\n\n## Processing\n\n- The workflow will remove nextera adapters and low quality bases and filter out any read smaller than 15bp.\n- The filtered reads are mapped with bowtie2 allowing dovetail and fragment length up to 1kb.\n- The BAM is filtered to keep only MAPQ30, concordant pairs and pairs outside of the mitochondria.\n- The PCR duplicates are removed with Picard (only from version 0.8).\n- The BAM is converted to BED to enable macs2 to take both pairs into account.\n- The peaks are called with macs2 which at the same time generates a coverage file.\n- The coverage file is converted to bigwig\n- The amount of reads 500bp from summits and the total number of reads are computed.\n- Two normalizations are computed:\n  - By million reads\n  - By million reads in peaks (500bp from summits)\n- Other QC are performed:\n  - A histogram with fragment length is computed.\n  - The evaluation of percentage of reads to chrM or MT is computed.\n- A multiQC is run to have an overview of the QC.\n\n### Warning\n\n- The `reference_genome` parameter value is used to select references in bowtie2 and bedtools slopbed. Only references that are present in bowtie2 **and** bedtools slopbed are selectable. If your favorite reference genome is not available ask your administrator to make sure that each bowtie2 reference has a corresponding len file for use in bedtools slopbed.\n",
        "updated": "2024-11-28T00:00:00",
        "categories": [],
        "collections": [
            "Epigenetics"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/chipseq-sr/main",
        "iwcID": "chipseq-sr-main",
        "readme": "# ChIP-seq Analysis: Single-End Read Processing\n\n## Inputs dataset\n\n- The workflow needs a single input which is a list of fastqsanger files.\n\n## Inputs values\n\n- adapters sequence_forward: this depends on the library preparation. If you don't know, use FastQC to determine if it is Truseq or Nextera.\n- reference_genome: this field will be adapted to the genomes available for bowtie2.\n- effective_genome_size: this is used by MACS2 and may be entered manually (indications are provided for heavily used genomes).\n- normalize_profile: Whether you want to have a profile normalized as Signal to Million Reads.\n\n## Processing\n\n- The workflow will remove illumina adapters and low quality bases and filter out any read smaller than 15bp.\n- The filtered reads are mapped with bowtie2 with default parameters.\n- The BAM is filtered to keep only MAPQ30.\n- The peaks are called with MACS2 with a fixed extension of 200bp which at the same time generates a coverage file (normalized or not).\n- The coverage is converted to bigwig.\n- A MultiQC is run to have an overview of the QC.\n\n### Warning\n\n- The filtered bam still has PCR duplicates which are removed by MACS2.\n",
        "updated": "2025-03-10T00:00:00",
        "categories": [],
        "collections": [
            "Epigenetics"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/cutandrun/main",
        "iwcID": "cutandrun-main",
        "readme": "# CUT&RUN/CUT&TAG Analysis: Protein-DNA Interaction Mapping\n\n## Inputs dataset\n\n- The workflow needs a single input which is a list of dataset pairs of fastqsanger.\n\n## Inputs values\n\n- adapter sequences: this depends on the library preparation. Usually CUT&RUN is Truseq and CUT&TAG is Nextera. If you don't know, use FastQC to determine if it is Truseq or Nextera\n- reference_genome: this field will be adapted to the genomes available for bowtie2\n- effective_genome_size: this is used by macs2 and may be entered manually (indications are provided for heavily used genomes)\n- normalize_profile: Whether you want to have a profile normalized as Signal to Million Reads.\n\n## Processing\n\n- The workflow will remove illumina adapters and low quality bases and filter out any read smaller than 15bp\n- The filtered reads are mapped with bowtie2 allowing dovetail and fragment length up to 1kb\n- The BAM is filtered to keep only MAPQ30 and concordant pairs\n- The PCR duplicates are removed with Picard (only from version 0.6)\n- The BAM is converted to BED to enable macs2 to take both pairs into account\n- The peaks are called with macs2 which at the same time generates a coverage file (normalized or not).\n- The coverage file is converted to bigwig\n- A multiQC is run to have an overview of the QC\n",
        "updated": "2025-01-27T00:00:00",
        "categories": [],
        "collections": [
            "Epigenetics"
        ]
    },
    {
        "name": "consensus-peaks-chip-sr",
        "trsID": "#workflow/github.com/iwc-workflows/consensus-peaks/consensus-peaks-chip-sr",
        "iwcID": "consensus-peaks-consensus-peaks-chip-sr",
        "readme": "# Consensus Peak Calling for ChIP-seq, ATAC-seq and CUT&RUN Replicates\n\nThe goal of this workflow is to get a list of confident peaks with summits from n replicates.\n\n## Inputs dataset\n\n- The workflow needs a single input which is a list of datasets with n BAM where PCR duplicates have been removed (the workflow also works for nested list if you have multiple conditions each with multiple replicates).\n\n## Inputs values\n\n- Minimum number of overlap: Minimum number of replicates into which the final summit should be present.\n- effective_genome_size: this is used by MACS2 and may be entered manually (indications are provided for heavily used genomes).\n- bin_size: this is the bin sized used to compute the average of normalized profiles. Large values will allow to have a smaller output file but with less resolution while small values will increase computation time and size of the output file to produce a more resolutive bigwig.\n\n## Strategy summary\n\nHere is a generated example to highlight the strategy:\n![strategy](https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/epigenetics/consensus-peaks/strategy.png)\n\n## Processing\n\n- The workflow will:\n  - first part:\n    - call peaks and compute normalized coverage on each BAM individually\n    - average normalized profiles\n    - compute the intersection between all peaks and filter when at least x replicate overlaps\n  - second part:\n    - subset all BAM to get the same number of reads\n    - call peaks on all subsetted BAM combined\n  - finally, keep only peaks from the second part that have summits overlapping the filtered intersection of the first part.\n",
        "updated": "2025-01-27T00:00:00",
        "categories": [],
        "collections": [
            "Epigenetics"
        ]
    },
    {
        "name": "consensus-peaks-chip-pe",
        "trsID": "#workflow/github.com/iwc-workflows/consensus-peaks/consensus-peaks-chip-pe",
        "iwcID": "consensus-peaks-consensus-peaks-chip-pe",
        "readme": "# Consensus Peak Calling for ChIP-seq, ATAC-seq and CUT&RUN Replicates\n\nThe goal of this workflow is to get a list of confident peaks with summits from n replicates.\n\n## Inputs dataset\n\n- The workflow needs a single input which is a list of datasets with n BAM where PCR duplicates have been removed (the workflow also works for nested list if you have multiple conditions each with multiple replicates).\n\n## Inputs values\n\n- Minimum number of overlap: Minimum number of replicates into which the final summit should be present.\n- effective_genome_size: this is used by MACS2 and may be entered manually (indications are provided for heavily used genomes).\n- bin_size: this is the bin sized used to compute the average of normalized profiles. Large values will allow to have a smaller output file but with less resolution while small values will increase computation time and size of the output file to produce a more resolutive bigwig.\n\n## Strategy summary\n\nHere is a generated example to highlight the strategy:\n![strategy](https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/epigenetics/consensus-peaks/strategy.png)\n\n## Processing\n\n- The workflow will:\n  - first part:\n    - call peaks and compute normalized coverage on each BAM individually\n    - average normalized profiles\n    - compute the intersection between all peaks and filter when at least x replicate overlaps\n  - second part:\n    - subset all BAM to get the same number of reads\n    - call peaks on all subsetted BAM combined\n  - finally, keep only peaks from the second part that have summits overlapping the filtered intersection of the first part.\n",
        "updated": "2025-01-27T00:00:00",
        "categories": [],
        "collections": [
            "Epigenetics"
        ]
    },
    {
        "name": "consensus-peaks-atac-cutandrun",
        "trsID": "#workflow/github.com/iwc-workflows/consensus-peaks/consensus-peaks-atac-cutandrun",
        "iwcID": "consensus-peaks-consensus-peaks-atac-cutandrun",
        "readme": "# Consensus Peak Calling for ChIP-seq, ATAC-seq and CUT&RUN Replicates\n\nThe goal of this workflow is to get a list of confident peaks with summits from n replicates.\n\n## Inputs dataset\n\n- The workflow needs a single input which is a list of datasets with n BAM where PCR duplicates have been removed (the workflow also works for nested list if you have multiple conditions each with multiple replicates).\n\n## Inputs values\n\n- Minimum number of overlap: Minimum number of replicates into which the final summit should be present.\n- effective_genome_size: this is used by MACS2 and may be entered manually (indications are provided for heavily used genomes).\n- bin_size: this is the bin sized used to compute the average of normalized profiles. Large values will allow to have a smaller output file but with less resolution while small values will increase computation time and size of the output file to produce a more resolutive bigwig.\n\n## Strategy summary\n\nHere is a generated example to highlight the strategy:\n![strategy](https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/epigenetics/consensus-peaks/strategy.png)\n\n## Processing\n\n- The workflow will:\n  - first part:\n    - call peaks and compute normalized coverage on each BAM individually\n    - average normalized profiles\n    - compute the intersection between all peaks and filter when at least x replicate overlaps\n  - second part:\n    - subset all BAM to get the same number of reads\n    - call peaks on all subsetted BAM combined\n  - finally, keep only peaks from the second part that have summits overlapping the filtered intersection of the first part.\n",
        "updated": "2025-01-27T00:00:00",
        "categories": [],
        "collections": [
            "Epigenetics"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/chipseq-pe/main",
        "iwcID": "chipseq-pe-main",
        "readme": "# ChIP-seq Analysis: Paired-End Read Processing\n\n## Inputs dataset\n\n- The workflow needs a single input which is a list of dataset pairs of fastqsanger.\n\n## Inputs values\n\n- adapters sequences: this depends on the library preparation. If you don't know, use FastQC to determine if it is Truseq or Nextera.\n- reference_genome: this field will be adapted to the genomes available for bowtie2.\n- effective_genome_size: this is used by MACS2 and may be entered manually (indications are provided for heavily used genomes).\n- normalize_profile: Whether you want to have a profile normalized as Signal to Million Fragments.\n\n## Processing\n\n- The workflow will remove illumina adapters and low quality bases and filter out any pair with mate smaller than 15bp.\n- The filtered reads are mapped with bowtie2 with default parameters.\n- The BAM is filtered to keep only MAPQ30 and concordant pairs.\n- The peaks are called with MACS2 which at the same time generates a coverage file (normalized or not).\n- The coverage is converted to bigwig.\n- A MultiQC is run to have an overview of the QC.\n\n### Warning\n\n- The filtered bam still has PCR duplicates which are removed by MACS2.\n\n## Contribution\n\n@lldelisle wrote the workflow.\n\n@nagoue updated the tools, made it work in usegalaxy.org, fixed the best practices and wrote the tests.\n",
        "updated": "2025-03-10T00:00:00",
        "categories": [],
        "collections": [
            "Epigenetics"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/average-bigwig-between-replicates/main",
        "iwcID": "average-bigwig-between-replicates-main",
        "readme": "# BigWig Replicates Averaging Workflow\n\nThis workflow is very useful when you processed multiple samples in collections and you want to generate an average coverage per condition.\n\n## Inputs dataset\n\n- The workflow needs a single input which is a list of bigwigs (normalized). The identifiers of your bigwigs must be like:\n  - whatever_sample1_identificationOfReplicate1\n  - whatever_sample1_identificationOfReplicate2\n  - ...\n  - whatever_sample2_identificationOfReplicate1\n  - whatever_sample2_identificationOfReplicate2\n  - ...\n\n## Inputs values\n\n- bin_size: this is used when average of coverage is performed. Large values will allow to have smaller output files but with less resolution while small values will increase computation time and size of output files to produce more resolutive bigwigs. I suggest 5bp for RNA-seq and 50bp for other applications.\n\n## Processing\n\n- The workflow will split identifiers between everything which is before the last underscore which will be the *sample* and everything which is after the last underscore which will be the *replicate identifier*. And restructure the collection as list:list:\n  - whatever_sample1:\n    - identificationOfReplicate1\n    - identificationOfReplicate2\n    - ...\n  - whatever_sample2:\n    - identificationOfReplicate1\n    - identificationOfReplicate2\n    - ---\n  - ...\n- Then it will average bigwigs into each inner list\n\n## Outputs\n\n- The output is a collection of bigwig datasets like:\n  - whatever_sample1\n  - whatever_sample2\n  - ...\n",
        "updated": "2023-09-27T00:00:00",
        "categories": [],
        "collections": [
            "Epigenetics",
            "Transcriptomics"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/generic-variant-calling-wgs-pe/main",
        "iwcID": "generic-variant-calling-wgs-pe-main",
        "readme": "Generic variation analysis on WGS PE data\n-------------------------------------------\n\nThis workflows performs paired end read mapping with bwa-mem followed by\nsensitive variant calling across a wide range of AFs with lofreq and variant\nannotation with snpEff. The reference genome can be provided as a GenBank file.\n",
        "updated": "2023-11-20T00:00:00",
        "categories": [
            "Variant Calling"
        ],
        "collections": [
            "Variant Calling"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/variation-reporting/main",
        "iwcID": "variation-reporting-main",
        "readme": "Generic variation analysis reporting\n--------------------------------------\n\nThis workflow takes table of variants produced by any of the variant calling workflows in\nhttps://github.com/galaxyproject/iwc/tree/main/workflows/variant-calling\nand generates a list of variants by Samples and by Variant.\n",
        "updated": "2023-11-20T00:00:00",
        "categories": [],
        "collections": [
            "Variant Calling"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/haploid-variant-calling-wgs-pe/main",
        "iwcID": "haploid-variant-calling-wgs-pe-main",
        "readme": "# Haploid variant calling for whole genome sequencing paired end data\n\nThis workflow uses Illumina or Element read data to discover variants (short nucleotide polymorphisms, SNPs, and small indels) in haploid genomes with multiple genomic sequences (contigs, scaffolds, or chromosomes).\n\n## Inputs dataset\n\n- The workflow needs a list of paired end fastq files\n- A GTF containtaing the Gene annotation for the selected haploid genome\n- A fasta file for the haploid genome to call variants against\n\n## Outputs\n\n- Tab-delimited summary of annotated variants\n- Report summarizing the quality of input data and mapping results\n\n## Processing\n\n- The workflow will remove adapters using fastp\n- The filtered reads are aligned with bwa-mem.\n- Only properly aligned mate pairs are retained, PCR duplicates are removed.\n- Alignments are re-aligned using lofreq viterbi and variants are called with lofreq call.\n- Variants are annotated with snpeff eff\n",
        "updated": "2024-10-29T00:00:00",
        "categories": [],
        "collections": [
            "Variant Calling"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/Assembly-decontamination-VGP9/main",
        "iwcID": "assembly-decontamination-vgp9-main",
        "readme": "## Decontamination Workflow\n\nDecontamination (foreign contaminants and mitochondrial sequences) of genome assembly after scaffolding step. Part of the VGP Suite. \n\n### Inputs\n\n- Genome Assembly [fasta]\n- Database for Kraken2. Database containing the possible contaminants.\n\n### Ouput\n\n- List of contaminant scaffolds\n- List of mitochondrial scaffolds\n- Decontaminated assembly\n",
        "updated": "2025-06-09T00:00:00",
        "categories": [],
        "collections": [
            "Vertebrate Genome Project"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/Purge-duplicate-contigs-VGP6/main",
        "iwcID": "purge-duplicate-contigs-vgp6-main",
        "readme": "# Purge Duplicate Contigs\n\nPurge contigs marked as duplicates by purge_dups (could be haplotypic duplication or overlap duplication). The contigs are purged from the first assembly (hap1, pri...), added to the second assembly (hp2, alt... ), then the 2nd assembly is purged as well. If you think only one of the assemblies needs purging, use the VGP6b workflow. \nThis workflow is the 6th workflow of the VGP pipeline. It is meant to be run after one of the contigging steps (Workflow 3, 4, or 5)\n\n## Inputs\n\n1. Hifi long reads - trimmed [fastq] (Generated by Cutadapt in the contigging workflow)\n2. Primary Assembly (hap1) [fasta] (Generated by the contigging workflow)\n3. Alternate Assembly (hap2) [fasta] (Generated by the contigging workflow)\n4. K-mer database [meryldb]  (Generated by the k-mer profiling workflow)\n5. Genomescope model parameters [txt] (Generated by the k-mer profiling workflow)\n6. Estimated Genome Size [txt]\n7. Database for busco lineage (recommended: latest) \n8. Lineage of your species for Busco Orthologs (recommended: vertebrata)\n9. Name of first haplotype\n10. Name of second haplotype\n\n\n## Outputs\n\n1. Haplotype 1 purged assembly (Fasta and gfa)\n2. Haplotype 2 purged assembly (Fasta and gfa)\n3. QC: BUSCO report for both assemblies\n4. QC: Compleasm report for both assemblies\n5. QC: Merqury report for both assemblies\n6. QC: Assembly statistics for both assemblies\n7. QC: Nx plot for both assemblies\n8. QC: Size plot for both assemblies\n",
        "updated": "2025-06-09T00:00:00",
        "categories": [],
        "collections": [
            "Vertebrate Genome Project"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/kmer-profiling-hifi-VGP1/main",
        "iwcID": "kmer-profiling-hifi-vgp1-main",
        "readme": "# VGP Workflow #1\n\nThis workflow produces a Meryl database and Genomescope outputs that will be used to determine parameters for following workflows, and assess the quality of genome assemblies. Specifically, it provides information about the genomic complexity, such as the genome size and levels of heterozygosity and repeat content, as well about the data quality. It also provides statistics on the PacBio Hifi reads. \n\n### Inputs\n\n1. The name of the species being assembled\n2. The Name of the assembly\n3. A collection of Hifi long reads in FASTQ format\n4. *k*-mer length\n5. Ploidy\n\n### Outputs\n\n-   Meryl Database of *k*-mer counts\n-   GenomeScope\n    -   Linear plot\n    -   Log plot\n    -   Transformed linear plot\n    -   Transformed log plot\n    -   Summary\n    -   Model\n    -   Model parameteres\n- RDeval for PacBio Hifi Reads QC\n    -   Reads statistics\n    -   HTML report\n  \n\n ![image](https://github.com/galaxyproject/iwc/assets/4291636/565238fc-f8a9-46ac-8b31-6276410fa436)\n",
        "updated": "2025-06-09T00:00:00",
        "categories": [],
        "collections": [
            "Vertebrate Genome Project"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/Mitogenome-assembly-VGP0/main",
        "iwcID": "mitogenome-assembly-vgp0-main",
        "readme": "# Assembly of Mitochondrial DNA from PacBio HiFi reads\n\nGenerate mitochondrial assembly based on PacBio HiFi reads. Part of the VGP suite, it can be run at any time independently of the other workflows. This workflow uses MitoHiFi and a mitochondrial reference to assemble the mitochondrial genome from PacBio reads. You do not need to provide the reference yourself, only the Latin name of the species.\n\n\n## Inputs\n\n1. Name of the Species\n2. Name of the Assembly\n3. Hifi long reads [fastq]\n4. Email adress required for NCBI database query \n\n## Outputs\n\n1. Contigs Statistics\n2. Images : \n   1. Mitogenome Coverage\n   2. Mitogenome Annotation\n3. Genbank file of the assembled mitogenome\n4. Fasta file of the assembled mitogenome",
        "updated": "2025-05-16T00:00:00",
        "categories": [],
        "collections": [
            "Vertebrate Genome Project"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/Assembly-Hifi-only-VGP3/main",
        "iwcID": "assembly-hifi-only-vgp3-main",
        "readme": "## Contiging Solo:\n\nGenerate assembly based on PacBio Hifi Reads.\n\n\n### Inputs\n\n\n1. Hifi long reads [fastq]\n2. K-mer database [meryldb]\n3. Genome profile summary generated by Genomescope [txt]\n4. Homozygous Read Coverage. Optional, use if you think the estimation from Genomescope is inacurate. \n5. Genomescope Model Parameters generated by Genomescope [tabular]\n6. Database for busco lineage (recommended: latest)\n7. Busco lineage (recommended: vertebrata)\n8. Name of first assembly\n9. Name of second assembly\n\n\n### Outputs\n\n1. Primary assembly\n2. Alternate assembly\n3. QC: Bandage image for the raw unitigs\n4. QC: BUSCO report for both assemblies\n5. QC: Compleasm report for both assemblies\n6. QC: Merqury report for both assemblies\n7. QC: Assembly statistics for both assemblies\n8. QC: Nx plot for both assemblies\n9. QC: Size plot for both assemblie\n",
        "updated": "2025-06-09T00:00:00",
        "categories": [],
        "collections": [
            "Vertebrate Genome Project"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/Scaffolding-Bionano-VGP7/main",
        "iwcID": "scaffolding-bionano-vgp7-main",
        "readme": "# Scaffolding with Bionano\n\nScaffolding using Bionano optical map data\n\n## Inputs\n\n1. Bionano data [cmap]\n2. Estimated genome size [txt]\n3. Phased assembly generated by Hifiasm [gfa1]\n\n## Outputs\n\n1. Scaffolds\n2. Non-scaffolded contigs\n3. QC: Assembly statistics\n4. QC: Nx plot\n5. QC: Size plot",
        "updated": "2024-08-13T00:00:00",
        "categories": [],
        "collections": [
            "Vertebrate Genome Project"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/hi-c-contact-map-for-assembly-manual-curation/main",
        "iwcID": "hi-c-contact-map-for-assembly-manual-curation-main",
        "readme": "# Hi-C Contact map generation for manual curation of genome assemblies\n\nThis workflow generates Hi-C contact maps for diploid genome assemblies in the Pretext format. It includes tracks for PacBio read coverage, Gaps, and telomeres. The Pretext files can be open in PretextView for the manual curation of genome assemblies. \n\n\n## Inputs\n\n1. **Haplotype 1** [fasta]\n2. **Will you use a second haplotype?** \n3. **Haplotype 2** [fasta]\n4. **Do you want to add suffixes to the scaffold names?** Select yes if the scaffold names in your assembly do not contain haplotype information.\n5. **Haplotype 1 suffix** This suffix will be added to haplotype 1 scaffold names if you selected to add suffixes to the scaffold names.\n6. **Haplotype 2 suffix** This suffix will be added to haplotype 2 scaffold names if you selected to add suffixes to the scaffold names.\n7. **Hi-C reads**  [fastq] Paired Collection containing the Hi-D data\n8. **Do you want to trim the Hi-C data?** If *yes*, remove 5bp at the end of Hi-C reads. Use with Arima Hi-C data if the Hi-C map looks \"noisy\".\n9. **Minimum Mapping Score** Minimum mapping score to keep for Hi-C alignments in the filtered PretextMap. Set to 0 to keep all mapped reads. Default: 20 .\n10. **Telomere repeat to suit species** Expected value of the repeated sequences in the telomeres. Default value [CCCTAA] is suited to vertebrates.\n11. **PacBio reads** [fastq] Collection of PacBio reads.\n\n\n## Outputs\n\n1. Concatenated Assembly [fasta] If two haplotypes are used. \n2. Trimmed Hi-C data (If trimming option is selected) [fastq]\n3. Mapped Hi-C reads [bam]\n4. Telomeres track [bedgraph]\n5. Gap track [bedgraph] \n6. Coverage track [bigwig]\n7. Gaps in coverage track [bedgraph]\n7. Pretext Map without tracks [pretext], filtered and unfiltered.\n8. Pretext Map with tracks [pretext], filtered and unfiltered.\n9. Pretext Snapshot image of the Hi-C contact map [png], filtered and unfiltered.\n",
        "updated": "2025-06-09T00:00:00",
        "categories": [],
        "collections": [
            "Vertebrate Genome Project"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/kmer-profiling-hifi-trio-VGP2/main",
        "iwcID": "kmer-profiling-hifi-trio-vgp2-main",
        "readme": "# VGP Workflow #1\n\nThis workflow collects the metrics on the properties of the genome under consideration by analyzing the *k*-mer frequencies. It provides information about the genomic complexity, such as the genome size and levels of heterozygosity and repeat content, as well about the data quality. It uses reads from two parental genomes to partition long reads from the offspring into haplotype-specific *k*-mer databases.\n\n### Inputs\n\n-   Collection of Hifi long reads [fastq] (Collection)\n-   Paternal short-read Illumina sequencing reads [fastq] (Collection)\n-   Maternal short-read Illumina sequencing reads [fastq] (Collection)\n-   *k*-mer length\n-   Ploidy\n\n### Outputs\n\n-   Meryl databases of k-mer counts\n    - Child\n    - Paternal haplotype\n    - Maternal haplotype\n-   GenomeScope metrics for child and the two parental genomes (three GenomeScope profiles in total)\n    -   Linear plot\n    -   Log plot\n    -   Transformed linear plot\n    -   Transformed log plot\n    -   Summary\n    -   Model\n    -   Model parameteres\n \n    ![image](https://github.com/galaxyproject/iwc/assets/4291636/35282f8e-d021-44f6-8e03-7b58b32d6d00)\n",
        "updated": "2025-04-30T00:00:00",
        "categories": [],
        "collections": [
            "Vertebrate Genome Project"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/Purge-duplicates-one-haplotype-VGP6b/main",
        "iwcID": "purge-duplicates-one-haplotype-vgp6b-main",
        "readme": "# Purge Duplicate Contigs\n\nPurge contigs marked as duplicates by purge_dups in a single haplotype (could be haplotypic duplication or overlap duplication). If you think the purged contigs might belong to the other haplotype, use the workflow VGP6 instead. \nThis workflow is the 6th workflow of the VGP pipeline. It is meant to be run after one of the contigging steps (Workflow 3, 4, or 5).\n\n## Inputs\n\n1. Genomescope model parameters [txt] (Generated by the k-mer profiling workflow)\n2. Hifi long reads - trimmed [fastq] (Generated by Cutadapt in the contigging workflow)\n3. Assembly to purge (e.g. hap1) [fasta] (Generated by the contigging workflow)\n4. K-mer database [meryldb]  (Generated by the k-mer profiling workflow)\n5. Assembly to leave alone (used for merqury statistics) (e.g. hap2) [fasta] (Generated by the contigging workflow)\n6. Estimated Genome Size [txt]\n7. Database for busco lineage (recommended: latest)\n8. Busco lineage (recommended: vertebrata)\n9. Name of un-altered assembly\n10. Name of purged assembly\n\n\n## Outputs\n\n1. Purged assembly (Fasta and gfa)\n2. QC: BUSCO report for the purged assembly\n3. QC: Compleasm report for the purged assembly\n4. QC: Merqury report for both assemblies\n5. QC: Assembly statistics for both assemblies\n6. QC: Nx plot for both assemblies\n7. QC: Size plot for both assemblies\n",
        "updated": "2025-06-09T00:00:00",
        "categories": [],
        "collections": [
            "Vertebrate Genome Project"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/Assembly-Hifi-HiC-phasing-VGP4/main",
        "iwcID": "assembly-hifi-hic-phasing-vgp4-main",
        "readme": "# Genome Assembly from Hifi reads with HiC phasing - VGP4\n\nGenerate phased assembly based on PacBio Hifi Reads using HiC data from the same individual for assembly phasing. Part of the VGP workflow suite, it needs to be run after the k-mer profiling workflow VGP1.\n\n## Inputs\n\n1. Hifi long reads [fastq].\n2. Trim Hi-C reads ? If yes, trim 5 bases at the beginning of each reads. Use with some Arima Hi-C data if the contact map looks \"noisy\". \n3. Paired collection of Hi-C reads [fastq].\n4. Genome profile summary generated by Genomescope [txt] generated by VGP1 workflow.\n5. K-mer database [meryldb] generated by VGP1 workflow.\n6. Database to use for Busco lineages. Recommended : latest version.\n7. Lineage. Select the taxonomic lineage of the assembled species.  \n8. Name of first assembly.\n9. Name of second assembly.\n10. Bits for bloom filter. Change for large genomes to save memory.\n11. Homozygous Read Coverage. Optional: specify if the coverage detected by Genomescope in VGP1 in not satisfactory.\n12. Genomescope model parameters [tabular] generated by VGP1 workflow.\n\n## Outputs\n\n1. Haplotype 1 assembly ([fasta] and [gfa])\n2. Haplotype 2 assembly ([fasta] and [gfa])\n3. Trimmed Hi-C reads collection\n4. QC: MultiQC report for HiFi reads trimming\n5. QC: BUSCO report for both assemblies\n6. QC: Compleasm report for both assemblies\n7. QC: Merqury report for both assemblies\n8. QC: Assembly statistics for both assemblies\n9. QC: Nx plot for both assemblies\n10. QC: Size plot for both assemblies\n",
        "updated": "2025-06-09T00:00:00",
        "categories": [],
        "collections": [
            "Vertebrate Genome Project"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/Scaffolding-HiC-VGP8/main",
        "iwcID": "scaffolding-hic-vgp8-main",
        "readme": "# Scaffolding with HiC data\n\nThis workflow performs genome assembly scaffolding using HiC data with YAHS. It is designed to be run as part of the VGP analysis trajectories, but can be used on any assembly in GFA format with Hi-C data. To generate a GFA from a fasta assembly, you can use the gfastats tool in Galaxy.  \n\nExample of VGP trajectory : \n- VGP1: Kmer profiling \n- VGP4: Genome assembly with HiC phasing\n- VGP6: Purge duplicated haplotigs\n- VGP8: Scaffolding with HiC\n\n## Inputs\n\n1. Genome assembly [gfa]\n2. Haplotype being scaffolded (Will be added to scaffold names: e.g. `>scaffold_01_H1`)\n3. HiC reads paired collection [fastq]\n5. Trim Hi-C data? If `yes`, trim five bases at the beginning of each read. Use with Arima Hi-C data if the Hi-C map looks \"noisy\" and the reads haven't been trimmed before. \n6. Minimum Mapping Quality [int] (Default:20). Minimum mapping quality for Hi-C alignments. Set to 0 if you want no filtering.  \n6. Database for busco lineage (recommended: latest)\n7. Busco lineage (recommended for VGP data: vertebrata)\n8. Restriction enzyme sequence (recommended for VGP data: Arima Hi-C 2.0)\n9. Estimated genome size [txt] (Output from the contigging workflows 3,4, or 5). A simple text file containing the estimated genome size as an integer. E.g. `2288021`\n\n\n### Outputs\n\n1. Scaffolds in [fasta] and [gfa] format with the haplotype in the scaffold names.\n2. If you selected `yes` for Hi-C trimming, the trimmed collections of Hi-C reads.\n3. QC: Assembly statistics.\n4. QC: Nx plot.\n5. QC: Size plot.\n6. QC: BUSCO report.\n7. QC: Compleasm report.\n8. QC: Pretext Maps before and after scaffolding.\n9. QC: Statistics on Hi-C alignements before and after scaffolding\n",
        "updated": "2025-06-09T00:00:00",
        "categories": [],
        "collections": [
            "Vertebrate Genome Project"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/Plot-Nx-Size/main",
        "iwcID": "plot-nx-size-main",
        "readme": "## Generate Nx and Size plot for multiple assemblies\n\nGenerate Nx and size plots for multiple assemblies to compare the evolution of assembly quality through the scaffolding process. Inputs are the fasta files for each assembly to compare.\n\n### Inputs\n\nCollection of fasta files. The name of each item in the collection will be used as labels for the Nx and Size plots.\n\n### Outputs\n\n\n1. Nx plot \n2. Size plot \n",
        "updated": "2025-06-09T00:00:00",
        "categories": [],
        "collections": [
            "Vertebrate Genome Project"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/Assembly-Hifi-Trio-phasing-VGP5/main",
        "iwcID": "assembly-hifi-trio-phasing-vgp5-main",
        "readme": "# Genome Assembly with Hifi reads and Trio Data\n\nGenerate phased assembly based on PacBio Hifi Reads using parental Illumina data for phasing. Part of the VGP workflow suite, it needs to be run after the Trio k-mer Profiling workflow VGP2.\n\n## Inputs\n\n1. Hifi long reads [fastq]\n2. Concatenated Illumina reads : Paternal [fastq]\n3. Concatenated Illumina reads : Maternal [fastq]\n4. K-mer database [meryldb] generated by VGP2 workflow.\n5. Paternal hapmer database [meryldb] generated by VGP2 workflow.\n6. Maternal hapmer database [meryldb] generated by VGP2 workflow.\n7. Bits for Bloom Filter. Change for large genomes to save memory.\n8. Database to use for Busco lineages. Recommended : latest version.\n8. Lineage. Select the taxonomic lineage of the assembled species.  \n9.  Homozygous read coverage (Estimated from the Genomescope model if not provided)\n10. Genome model parameters generated by Genomescope [tabular] generated by VGP2 workflow.\n11. Genome profile summary generated by Genomescope [txt] generated by VGP2 workflow.\n12. Name of first haplotype\n13. Name of second haplotype\n\n## Outputs\n\n1. Haplotype 1 assembly [fasta] and [gfa]\n2. Haplotype 2 assembly [fasta] and [gfa]\n9. QC: Size plot for both assemblies\n5. QC: MultiQC report for HiFi reads trimming\n6. QC: BUSCO report for both assemblies\n7. QC: Compleasm report for both assemblies\n8. QC: Merqury report for both assemblies\n9. QC: Assembly statistics for both assemblies\n10. QC: Nx plot for both assemblies\n11. QC: Size plot for both assemblies\n\n",
        "updated": "2025-06-09T00:00:00",
        "categories": [],
        "collections": [
            "Vertebrate Genome Project"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/fluorescence-nuclei-segmentation-and-counting/main",
        "iwcID": "fluorescence-nuclei-segmentation-and-counting-main",
        "readme": "# Segmentation and counting of cell nuclei in fluorescence microscopy images\n\nThis workflow performs segmentation and counting of cell nuclei using fluorescence microscopy images. The segmentation step is performed using Otsu thresholding (Otsu, 1979). The workflow is based on the tutorial: https://training.galaxyproject.org/training-material/topics/imaging/tutorials/imaging-introduction/tutorial.html\n\n![](test-data/overlay_image.png)\n\n## Inputs\n\n**`input_image`:** The fluorescence microscopy images to be segmented. Must be the single image channel, which contains the cell nuclei.\n\n## Outputs\n\n**`overlay_image`:** An overlay of the original image and the outlines of the segmentated objects, each also annotated with a unique number.\n\n**`objects_count`:** Table with a single column `objects` and a single row (the actual number of objects).\n\n**`label_image`:** The segmentation result (label map, which contains a unique label for each segmented object).\n",
        "updated": "2024-11-07T00:00:00",
        "categories": [],
        "collections": [
            "Imaging"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/tissue-microarray-analysis/main",
        "iwcID": "tissue-microarray-analysis-main",
        "readme": "# End-to-End Tissue Microarray Image Analysis with Galaxy-ME\n\n## Input datasets\n\n- Collection of raw cycle images (TIFF/OME-TIFF): Ensure that the list is ordered in cycle order (ex: cycle_1.tiff, cycle_2.tiff, etc.)\n- Markers file (CSV): A comma-separated file with `marker_names` in the third column\n\n    - Example markers file:\n\n```\nround,channel,marker_name\n0,0,DAPI_1\n0,1,CD3\n0,2,CD45\n0,3,CD8\n1,4,DAPI_2\n1,5,PANCK\n1,6,SMA\n1,7,ECAD\n...\n```\n\n\n- Phenotype file (CSV): A comma-separated Scimap phenotyping file that maps hierarchical cell phenotypes to markers\n\n    - For an example phenotype workflow, see our [tutorial](https://training.galaxyproject.org/training-material/topics/imaging/tutorials/multiplex-tissue-imaging-TMA/tutorial.html) and the [Scimap documentation](https://scimap-doc.readthedocs.io/en/latest/tutorials/scimap-tutorial-cell-phenotyping/).\n\n\n## Input values\n\nAll input values have been preset in the workflow and are optimized for cyclic immunofluorescence images captured using a Rarecyte slide scanner. Some important assumptions are made:\n\n- Channel used as a reference for registration (ASHLAR): `0`\n- Channel used for nuclear segmentation (Mesmer): `0`\n- Image resolution (microns per pixel): `0.65`\n\nThe workflow should be imported and edited if these values are not suitable for your datasets.\n\n## Processing\n\nFor more detailed information, see our [tutorial on the Galaxy Training Network](https://training.galaxyproject.org/training-material/topics/imaging/tutorials/multiplex-tissue-imaging-TMA/tutorial.html)\n\n- Tile-to-tile illumination differences are corrected in the unstitched input raw cycle images using **Basic Illumination**\n- A whole-slide OME-TIFF image is generated via stitching and registration with **ASHLAR**. Channel names are assigned at this step using the input markers file\n- TMA cores are segmented and cropped into individual images, producing a collection of TIFFs using **UNetCoreograph**. All subsequent steps are run as batch processing across the collection of core datasets\n- The output of **UNetCoreograph** is a generic TIFF, and must be converted back to OME-TIFF using the **Convert Image** tool, and channels can be renamed using the **Rename OME-TIFF channels** utility\n- Nuclear segmentation is performed using **Mesmer**, producing a nuclear mask in TIFF format for each core image\n- Cell/nuclear features (mean marker intensities, spatial coordinates, and morphological measurements) are quantified using **MCQUANT**, producing a CSV table of cells (rows) x features (columns)\n- The quantification table is converted to anndata format (h5ad), a common datatype used by most single-cell and spatial analysis packages\n- Automated cell phenotyping is performed using **Scimap** (see *Warning* section about GMM-based phenotyping)\n- Finally, **Vitessce** dashboards combine interactive image viewing with linked single-cell analysis components to allow for integrated initial data exploration\n\n## Warning\n\nIn this workflow, we perform automated GMM-based cell phenotyping using Scimap. The Scimap tool also accepts manual gates, which can be determined using the **GateFinder** tool. This method is highly recommended, as **most** markers are not well suited for GMM-based thresholding. The automated GMM-based thresholding can work well for highly abundant markers that show a strong bimodal distribution; otherwise, it should be used primarily as a means of generating an initial starting point for gating and cell phenotyping.\n\nFor more warnings and context, see our tutorial linked above.\n\n\n## Tool developers' documentation\n\n- [MCMICRO](https://mcmicro.org/)\n    - Basic Illumination\n    - ASHLAR\n    - UNetCoreograph\n    - MCQuant\n- [Mesmer](https://deepcell.readthedocs.io/en/master/)\n- [Scimap](https://scimap-doc.readthedocs.io/en/latest/)\n- [Vitessce](https://vitessce.io/)\n\n\n## Tool references\n\n- Peng, T., K. Thorn, T. Schroeder, L. Wang, F. J. Theis et al., 2017 A BaSiC tool for background and shading correction of optical microscopy images. Nature Communications 8: 10.1038/ncomms14836\n- Wolf, F. A., P. Angerer, and F. J. Theis, 2018 SCANPY: large-scale single-cell gene expression data analysis. Genome Biology 19: 10.1186/s13059-017-1382-0\n- Stringer, C., T. Wang, M. Michaelos, and M. Pachitariu, 2020 Cellpose: a generalist algorithm for cellular segmentation. Nature Methods 18: 100\u2013106. 10.1038/s41592-020-01018-x\n- Greenwald, N. F., G. Miller, E. Moen, A. Kong, A. Kagel et al., 2021 Whole-cell segmentation of tissue images with human-level performance using large-scale data annotation and deep learning. Nature Biotechnology 40: 555\u2013565. 10.1038/s41587-021-01094-0\n- Schapiro, D., A. Sokolov, C. Yapp, Y.-A. Chen, J. L. Muhlich et al., 2021 MCMICRO: a scalable, modular image-processing pipeline for multiplexed tissue imaging. Nature Methods 19: 311\u2013315. 10.1038/s41592-021-01308-y\nVirshup, I., S. Rybakov, F. J. Theis, P. Angerer, and F. A. Wolf, 2021 anndata: Annotated data. 10.1101/2021.12.16.473007\n- Muhlich, J. L., Y.-A. Chen, C. Yapp, D. Russell, S. Santagata et al., 2022 Stitching and registering highly multiplexed whole-slide images of tissues and tumors using ASHLAR (A. Valencia, Ed.). Bioinformatics 38: 4613\u20134621. 10.1093/bioinformatics/btac544\n- Palla, G., H. Spitzer, M. Klein, D. Fischer, A. C. Schaar et al., 2022 Squidpy: a scalable framework for spatial omics analysis. Nature Methods 19: 171\u2013178. 10.1038/s41592-021-01358-2\n- Yapp, C., E. Novikov, W.-D. Jang, T. Vallius, Y.-A. Chen et al., 2022 UnMICST: Deep learning with real augmentation for robust segmentation of highly multiplexed images of human tissues. Communications Biology 5: 10.1038/s42003-022-04076-3\n- Zhang, W., I. Li, N. E. Reticker-Flynn, Z. Good, S. Chang et al., 2022 Identification of cell types in multiplexed in situ images by combining protein expression and spatial information using CELESTA. Nature Methods 19: 759\u2013769. 10.1038/s41592-022-01498-z\n- Nirmal, A. J., and P. K. Sorger, 2024 SCIMAP: A Python Toolkit for Integrated Spatial Analysis of Multiplexed Imaging Data. Journal of Open Source Software 9: 6604. 10.21105/joss.06604\n",
        "updated": "2024-04-12T00:00:00",
        "categories": [],
        "collections": [
            "Imaging"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/bacterial_genome_annotation/main",
        "iwcID": "bacterial_genome_annotation-main",
        "readme": "# Bacterial genome annotation workflow (v1.1.8)\n\nThis workflow uses assembled bacterial genome fasta files (but can be any fasta file) and executes the following steps:\n1. Genomic annotation\n    - **Bakta** to predict CDS and small proteins (sORF)\n2. Integron identification\n    - **IntegronFinder2** to identify CALIN elements, In0 elements, and complete integrons\n3. Plasmid gene identification\n    - **Plasmidfinder** to identify and typing plasmid sequences\n4. Inserted sequence (IS) detection\n    - **ISEScan** to detect IS elements\n5. Aggregating outputs into a single JSON file\n    - **ToolDistillator** to extract and aggregate information from different tool outputs to JSON parsable files\n\n## Inputs\n\n1. Assembled bacterial genome in fasta format.\n\n## Outputs\n\n1. Genomic annotation:\n    - genome annotation in tabular, gff and several other formats\n    - annotation plot\n    - nucleotide and protein sequences identified\n    - summary of genomic identified elements\n2. Integron identification:\n    - integron identification in tabular format and a summary\n3. Plasmid gene identification:\n    - plasmid gene identified and associated blast hits\n4. Inserted Element (IS) detection:\n    - IS element list in tabular format\n    - is hits in fasta format\n    - ORF hits in protein and nucleotide fasta format\n    - IS annotation gff format\n5. Aggregating outputs:\n    - JSON file with information about the outputs of **Bakta**, **IntegronFinder2**, **Plasmidfinder**, **ISEScan**",
        "updated": "2025-04-15T00:00:00",
        "categories": [],
        "collections": [
            "Genome Annotation"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/amr_gene_detection/main",
        "iwcID": "amr_gene_detection-main",
        "readme": "# AMR gene detection workflow in an assembled bacterial genome (v1.0)\n\nThis workflow uses assembled bacterial genome fasta files (but can be any fasta file) and executes the following steps:\n1. Genomic detection\n    - Antimicrobial resistance gene identification:\n        - **staramr** to blast against ResFinder and PlasmidFinder database\n        - **AMRFinderPlus** to find antimicrobial resistance genes and point mutations \n    - Virulence gene identification:\n        - **ABRicate** with VFDB_A database\n2. Aggregating outputs into a single JSON file\n    - **ToolDistillator** to extract and aggregate information from different tool outputs to JSON parsable files\n\n## Inputs\n\n1. Assembled bacterial genome in fasta format.\n\n## Outputs\n\n1. Genomic detection\n    - Antimicrobial resistance gene identification:\n        - AMR gene list\n        - MLST typing\n        - Plasmid gene identification\n        - Blast hits\n        - AMR gene fasta (assembled nucleotide sequences)\n        - Point mutation list\n    - Virulence gene identification:\n        - Gene identification in tabular format\n2. Aggregating outputs:\n    - JSON file with information about the outputs of **staramr**, **AMRFinderPlus**, **ABRicate**",
        "updated": "2024-10-21T00:00:00",
        "categories": [],
        "collections": [
            "Genome Annotation"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/quality-and-contamination-control/main",
        "iwcID": "quality-and-contamination-control-main",
        "readme": "# Quality and Contamination control workflow for paired end data (v1.1.7)\n\nThis workflow uses paired-end illumina fastq(.gz) files and executes the following steps:\n1. Quality control and trimming\n    - **fastp** QC control and trimming\n2. Taxonomic assignation on trimmed data\n    - **Kraken2** assignation\n    - **Bracken** to re-estimate abundance to the species level\n    - **Recentrifuge** to make a krona chart\n3. Aggregating outputs into a single JSON file\n    - **ToolDistillator** to extract and aggregate information from different tool outputs to JSON parsable files\n\n## Inputs\n\n1. Paired-end illumina raw reads in fastq(.gz) format.\n\n## Outputs\n\n1. Quality control:\n    - quality report\n    - trimmed raw reads\n2. Taxonomic assignation:\n    - Tabular report of identified species\n    - Tabular file with assigned read to a taxonomic level\n    - Krona chart to illustrate species diversity of the sample\n3. Aggregating outputs:\n    - JSON file with information about the outputs of **fastp**, **Kraken2**, **Bracken**, **Recentrifuge** \n",
        "updated": "2025-05-05T00:00:00",
        "categories": [],
        "collections": [
            "Genome assembly"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/bacterial-genome-assembly/main",
        "iwcID": "bacterial-genome-assembly-main",
        "readme": "# Bacterial genome assembly workflow for paired end data (v1.1.6)\n\nThis workflow uses paired-end illumina trimmed reads fastq(.gz) files and executes the following steps:\n1. Assembly raw reads to a final contig fasta file \n    - **Shovill**\n2. Quality control of the assembly\n    - **Checkm2** to predict the completeness and contamination\n    - **Quast**\n    - **Bandage** to plot assembly graph\n    - **Refseqmasher** to identify the closed reference genome\n3. Aggregating outputs into a single JSON file\n    - **ToolDistillator** to extract and aggregate information from different tool outputs to JSON parsable files\n\n## Inputs\n\n1. Paired-end illumina trimmed reads in fastq(.gz) format.\n\n## Outputs\n\n1. Assembly:\n    - Assembly with contig in fasta\n    - Mapped read on assembly in bam format\n    - Graph assembly in gfa format\n2. Quality of Assembly:\n    - Expected completeness and contamination report\n    - Assembly report\n    - Assembly Graph\n    - Tabular result of closed reference genome\n3. Aggregating outputs\n    - JSON file with information about the outputs of **Shovill**, **Checkm2**, **Quast**, **Bandage**, **Refseqmasher**",
        "updated": "2025-05-14T00:00:00",
        "categories": [],
        "collections": [
            "Genome assembly"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/assembly-with-flye/main",
        "iwcID": "assembly-with-flye-main",
        "readme": "# Genome assembly with Flye workflow\n\n\n## Why use this workflow?\n\n- This is a fairly simple workflow that assembles a genome from long sequencing reads.\n- It takes in sequencing reads from PacBio (Hifi or non-Hifi), or Oxford Nanopore.\n- If you have PacBio Hifi reads, you may prefer to use a workflow with the assembly tool Hifiasm, such as the those in the suite of VGP workflows. \n\n## Inputs\n\nRaw sequencing reads from PacBio or Oxford Nanopore in format:\nfasta, fasta.gz, fastq, fastq.gz, fastqsanger.gz or fastqsanger\n\n## What does the workflow do\n\n- Assembles the reads with the tool Flye\n- Summarizes the statistics with the tool Fasta statistics\n- Report with the tool Quast\n- Renders the assembly graph with the tool Bandage\n\n## Settings\n\nRun as-is or change parameters at runtime\n\nFor example:\n- change the Flye option of \"mode\" to the correct sequencing type\n- change the Quast option for \"Type of organism\" to correct taxon\n \n## Outputs\n\n- Flye assembly output - four files: fasta, gfa for bandage, graph_dot file, assembly info\n- Fasta statistics\n- Bandage image\n- Quast report\n",
        "updated": "2024-03-25T00:00:00",
        "categories": [],
        "collections": [
            "Genome assembly"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/polish-with-long-reads/main",
        "iwcID": "polish-with-long-reads-main",
        "readme": "# Assembly polishing with Racon workflow\n\n## Inputs\n\n- Sequencing reads in format: fastq, fastq.gz, fastqsanger.gz or fastqsanger\n- Genome assembly to be polished, in fasta format\n\n## What does the workflow do\n\n- After long reads have been assembled into a genome (contigs), this can be polished with the same long reads. \n- This workflow uses the tool minimap2 to map the long reads back to the assembly, and then uses Racon to make polishes. \n- This is repeated a further 3 times. \n\nIn more detail:\n\n- minimap2 : long reads are mapped to assembly => overlaps.paf.\n- overaps, long reads, assembly => Racon => polished assembly 1\n- using polished assembly 1 as input; repeat minimap2 + racon => polished assembly 2\n- using polished assembly 2 as input, repeat minimap2 + racon => polished assembly 3\n- using polished assembly 3 as input, repeat minimap2 + racon => polished assembly 4\n\n## Settings\n\n- Run as-is or change parameters at runtime.\n- For the input at \"minimap settings for long reads\", enter (map-pb) for PacBio reads, (map-hifi) for PacBio HiFi reads, or (map-ont) for Oxford Nanopore reads.\n\n## Outputs\n\nThere is one output: the polished assembly in fasta format. \n\n",
        "updated": "2023-07-15T00:00:00",
        "categories": [],
        "collections": [
            "Genome assembly"
        ]
    },
    {
        "name": "scrna-seq-fastq-to-matrix-10x-cellplex",
        "trsID": "#workflow/github.com/iwc-workflows/fastq-to-matrix-10x/scrna-seq-fastq-to-matrix-10x-cellplex",
        "iwcID": "fastq-to-matrix-10x-scrna-seq-fastq-to-matrix-10x-cellplex",
        "readme": "# Single-Cell RNA-seq Preprocessing: 10X Genomics Data to Analysis-Ready Formats\n\nThese workflows are inspired by the [training material](https://training.galaxyproject.org/training-material/topics/single-cell/tutorials/scrna-preprocessing-tenx/tutorial.html). The output is in a 'bundle' format with three files (one matrix, one with genes, one with barcodes) which is similar to the cellranger output format, and is compatible with any Read10X function (Seurat or Scanpy). There are 2 types of output collections: either one collection with all matrices, one collection with all genes and one collection with all barcodes (compatible with post processing in galaxy) or a single nested collection with one sub-collection per sample with the 3 files (easier for local downstream analysis).\n\nBoth workflows are designed for fastqs from 10X libraries v3. One is for regular 10X library (one library per sample), while the other one is for CellPlex 10X library which allows to multiplex samples using CMOs (see [this blog article](https://www.10xgenomics.com/blog/answering-your-questions-about-sample-multiplexing-for-single-cell-gene-expression)).\n\n## Input datasets\n\n- Specific for each experiment:\n    - For both workflows: you need a list of pairs of fastqs with gene expression.\n    - For CellPlex: you need in addition a list of pairs of fastqs with CMO.\n    - For CellPlex: you need a list of csv which describes samples and CMO used:\n        - first column is the sequence and second column is the name\n    /!\\ The order of samples need to be exactly the same between the collection of fastqs of CMO and the collection of csv.\n\n- Common for all experiments:\n    - Gene annotations: A gtf file with gene locations\n    - List of barcodes used by 10X. You can download it at https://zenodo.org/record/3457880/files/3M-february-2018.txt.gz\n\n## Input values\n\n- reference genome: this genome needs to be available for STAR\n- Barcode Size is same size of the Read: if the length of your R1 of GEX matches the size of cell barcode + UMI set to true. If your R1 contains trailling A, put false.\n- number of cells: If you make it too large no cell barcode correction will be performed to demultiplex CMOs.\n\n## Processing\n- Gene expression processing:\n    - Reads are aligned to the genome, asigned to genes, cell barcode and UMI with STAR Solo\n    - MultiQC report the mapping rate and the number of reads attributed to genes\n    - The output of STAR Solo is filtered with Droplet Utils to remove cellular barcodes which are probably empty.\n    - The output of Droplet Utils is reorganized to be:\n```\nMain Collection:\n    - Sample 1:\n        - matrix.mtx\n        - barcodes.tsv\n        - genes.tsv\n    - Sample 2:\n        - matrix.mtx\n        - barcodes.tsv\n        - genes.tsv\n...\n```\nFor the CellPlex workflow:\n- CMO processing:\n    - CITE-Seq Count is used to asign reads and generate a matrix where 'genes' are the CMO and 'unmapped'.\n    - Cellular barcodes are translated to match the cellular barcodes of Gene expression see [this article](https://kb.10xgenomics.com/hc/en-us/articles/360031133451-Why-is-there-a-discrepancy-in-the-3M-february-2018-txt-barcode-whitelist-).\n    - Reorganize the output with UMI matrices to match the same structure as gene expression matrices.\n\n## Test data\n\nThe test dataset has been produced to make it as small as possible in order to make the workflow pass on CI.\n\n- The CMO reads come from [zenodo](https://zenodo.org/records/10229382) and have been sampled to 0.1 with seqtk.\n- The GEX reads come from SRR13948489 but have been subsetted to the cells selected in the above zenodo.\n",
        "updated": "2025-03-10T00:00:00",
        "categories": [],
        "collections": [
            "Transcriptomics",
            "Single Cell"
        ]
    },
    {
        "name": "scrna-seq-fastq-to-matrix-10x-v3",
        "trsID": "#workflow/github.com/iwc-workflows/fastq-to-matrix-10x/scrna-seq-fastq-to-matrix-10x-v3",
        "iwcID": "fastq-to-matrix-10x-scrna-seq-fastq-to-matrix-10x-v3",
        "readme": "# Single-Cell RNA-seq Preprocessing: 10X Genomics Data to Analysis-Ready Formats\n\nThese workflows are inspired by the [training material](https://training.galaxyproject.org/training-material/topics/single-cell/tutorials/scrna-preprocessing-tenx/tutorial.html). The output is in a 'bundle' format with three files (one matrix, one with genes, one with barcodes) which is similar to the cellranger output format, and is compatible with any Read10X function (Seurat or Scanpy). There are 2 types of output collections: either one collection with all matrices, one collection with all genes and one collection with all barcodes (compatible with post processing in galaxy) or a single nested collection with one sub-collection per sample with the 3 files (easier for local downstream analysis).\n\nBoth workflows are designed for fastqs from 10X libraries v3. One is for regular 10X library (one library per sample), while the other one is for CellPlex 10X library which allows to multiplex samples using CMOs (see [this blog article](https://www.10xgenomics.com/blog/answering-your-questions-about-sample-multiplexing-for-single-cell-gene-expression)).\n\n## Input datasets\n\n- Specific for each experiment:\n    - For both workflows: you need a list of pairs of fastqs with gene expression.\n    - For CellPlex: you need in addition a list of pairs of fastqs with CMO.\n    - For CellPlex: you need a list of csv which describes samples and CMO used:\n        - first column is the sequence and second column is the name\n    /!\\ The order of samples need to be exactly the same between the collection of fastqs of CMO and the collection of csv.\n\n- Common for all experiments:\n    - Gene annotations: A gtf file with gene locations\n    - List of barcodes used by 10X. You can download it at https://zenodo.org/record/3457880/files/3M-february-2018.txt.gz\n\n## Input values\n\n- reference genome: this genome needs to be available for STAR\n- Barcode Size is same size of the Read: if the length of your R1 of GEX matches the size of cell barcode + UMI set to true. If your R1 contains trailling A, put false.\n- number of cells: If you make it too large no cell barcode correction will be performed to demultiplex CMOs.\n\n## Processing\n- Gene expression processing:\n    - Reads are aligned to the genome, asigned to genes, cell barcode and UMI with STAR Solo\n    - MultiQC report the mapping rate and the number of reads attributed to genes\n    - The output of STAR Solo is filtered with Droplet Utils to remove cellular barcodes which are probably empty.\n    - The output of Droplet Utils is reorganized to be:\n```\nMain Collection:\n    - Sample 1:\n        - matrix.mtx\n        - barcodes.tsv\n        - genes.tsv\n    - Sample 2:\n        - matrix.mtx\n        - barcodes.tsv\n        - genes.tsv\n...\n```\nFor the CellPlex workflow:\n- CMO processing:\n    - CITE-Seq Count is used to asign reads and generate a matrix where 'genes' are the CMO and 'unmapped'.\n    - Cellular barcodes are translated to match the cellular barcodes of Gene expression see [this article](https://kb.10xgenomics.com/hc/en-us/articles/360031133451-Why-is-there-a-discrepancy-in-the-3M-february-2018-txt-barcode-whitelist-).\n    - Reorganize the output with UMI matrices to match the same structure as gene expression matrices.\n\n## Test data\n\nThe test dataset has been produced to make it as small as possible in order to make the workflow pass on CI.\n\n- The CMO reads come from [zenodo](https://zenodo.org/records/10229382) and have been sampled to 0.1 with seqtk.\n- The GEX reads come from SRR13948489 but have been subsetted to the cells selected in the above zenodo.\n",
        "updated": "2025-03-10T00:00:00",
        "categories": [],
        "collections": [
            "Transcriptomics",
            "Single Cell"
        ]
    },
    {
        "name": "baredSC-1d-logNorm",
        "trsID": "#workflow/github.com/iwc-workflows/baredsc/baredSC-1d-logNorm",
        "iwcID": "baredsc-baredsc-1d-lognorm",
        "readme": "# Single-Cell Mixture Analysis: baredSC Log-Normalized Models\n\nThese workflows allow to run a baredSC analysis from a table with counts in a single click. It uses models from 1 to N Gaussians and combine them. It uses the logNorm scale, 100 bins for 1 dimension and 25 bins on each axis in 2 dimensions.\n\n## Inputs dataset\n\n- Both workflows need a tabular dataset where each row is a cell. The tabular needs to have a header line with column names. There must be at least two columns: 'nCount_RNA' and another one with the counts for the gene(s) of interest. A way to get such table in R from a Seurat object (`seurat.obj`) is:\n\n```r\nmy.genes <- c(\"Hoxa13\", \"Hoxd13\")\ndf <- cbind(seurat.obj[[]], # This will give you all metadata including nCount_RNA\n            FetchData(seurat.obj, slot = \"counts\", vars = my.genes))\n\nwrite.table(df, \"input_for_baredSC.txt\", quote = F, sep = \"\\t\", row.names = F)\n```\n\n## Inputs values\n\nFor the 1D:\n\n- Gene name: The name of the column with the counts of your gene of interest.\n- Maximum value in logNorm: The maximum value to explore in PDF. This value should be large enough so the PDF is at 0 at this value.\n- Maximum number of Gaussians to study: All models between models with 1 Gaussians to models with this number of Gaussians will be combined.\n\nFor the 2D:\n\n- Gene name for x axis: The name of the column with the counts of your gene in x axis.\n- Gene name for y axis: The name of the column with the counts of your gene in y axis.\n- maximum value in logNorm for x-axis: The maximum value to explore in PDF in the x axis. This value should be large enough so the PDF is at 0 at this value.\n- maximum value in logNorm for y-axis: The maximum value to explore in PDF in the y axis. This value should be large enough so the PDF is at 0 at this value.\n- Maximum number of Gaussians to study: All models between models with 1 2D-Gaussians to models with this number of 2D-Gaussians will be combined.\n- compute p-value: Whether you want to get a p-value. As a consequence, less samples than available will be used for plots as p-value computation requires to have independent samples.\n\n## Processing\n\n- The workflow will generate paramater values from 1 to the maximum number of Gaussians to study.\n- baredSC_1d or baredSC_2d is run for each of these number of Gaussians\n- All models are combined into a single result.\n",
        "updated": "2025-03-10T00:00:00",
        "categories": [],
        "collections": [
            "Transcriptomics",
            "Single Cell"
        ]
    },
    {
        "name": "baredSC-2d-logNorm",
        "trsID": "#workflow/github.com/iwc-workflows/baredsc/baredSC-2d-logNorm",
        "iwcID": "baredsc-baredsc-2d-lognorm",
        "readme": "# Single-Cell Mixture Analysis: baredSC Log-Normalized Models\n\nThese workflows allow to run a baredSC analysis from a table with counts in a single click. It uses models from 1 to N Gaussians and combine them. It uses the logNorm scale, 100 bins for 1 dimension and 25 bins on each axis in 2 dimensions.\n\n## Inputs dataset\n\n- Both workflows need a tabular dataset where each row is a cell. The tabular needs to have a header line with column names. There must be at least two columns: 'nCount_RNA' and another one with the counts for the gene(s) of interest. A way to get such table in R from a Seurat object (`seurat.obj`) is:\n\n```r\nmy.genes <- c(\"Hoxa13\", \"Hoxd13\")\ndf <- cbind(seurat.obj[[]], # This will give you all metadata including nCount_RNA\n            FetchData(seurat.obj, slot = \"counts\", vars = my.genes))\n\nwrite.table(df, \"input_for_baredSC.txt\", quote = F, sep = \"\\t\", row.names = F)\n```\n\n## Inputs values\n\nFor the 1D:\n\n- Gene name: The name of the column with the counts of your gene of interest.\n- Maximum value in logNorm: The maximum value to explore in PDF. This value should be large enough so the PDF is at 0 at this value.\n- Maximum number of Gaussians to study: All models between models with 1 Gaussians to models with this number of Gaussians will be combined.\n\nFor the 2D:\n\n- Gene name for x axis: The name of the column with the counts of your gene in x axis.\n- Gene name for y axis: The name of the column with the counts of your gene in y axis.\n- maximum value in logNorm for x-axis: The maximum value to explore in PDF in the x axis. This value should be large enough so the PDF is at 0 at this value.\n- maximum value in logNorm for y-axis: The maximum value to explore in PDF in the y axis. This value should be large enough so the PDF is at 0 at this value.\n- Maximum number of Gaussians to study: All models between models with 1 2D-Gaussians to models with this number of 2D-Gaussians will be combined.\n- compute p-value: Whether you want to get a p-value. As a consequence, less samples than available will be used for plots as p-value computation requires to have independent samples.\n\n## Processing\n\n- The workflow will generate paramater values from 1 to the maximum number of Gaussians to study.\n- baredSC_1d or baredSC_2d is run for each of these number of Gaussians\n- All models are combined into a single result.\n",
        "updated": "2025-03-10T00:00:00",
        "categories": [],
        "collections": [
            "Transcriptomics",
            "Single Cell"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/scanpy-clustering/main",
        "iwcID": "scanpy-clustering-main",
        "readme": "# Single-Cell RNA-seq Analysis: Scanpy Preprocessing and Clustering\n\nThis workflow follows Scanpy legacy workflow [clustering 3k PBMCs](https://scanpy.readthedocs.io/en/stable/tutorials/basics/clustering-2017.html). For more details on concepts and parameters, please refer to the equivalent Galaxy-based [tutorial](https://training.galaxyproject.org/training-material/topics/single-cell/tutorials/scrna-scanpy-pbmc3k/tutorial.html).\n\n## Inputs\n\n### Input datasets\n- This workflow needs 3 files as input\n    - A single-cell count matrix file in Matrix Market Exchange format\n    - A cell barcodes file with a single barcode in each line. The barcodes should correspond to the cells in the matrix file\n    - A genes/features tabular file with gene ids and gene names.\n        - Cell Ranger v2 or earlier version call this file as `genes.tsv` and contains two columns:\n            - Gene ID (Ensembl gene ID or other identifiers)\n            - Gene Name (common gene name or symbol)\n        - Cell Ranger v2 or earlier version call this file as `features.tsv` and contains three columns:\n            - Feature ID (Ensembl gene ID or other identifiers)\n            - Feature Name (common gene name or symbol)\n            - Feature Type (e.g., Gene Expression, Antibody Capture, CRISPR Guide Capture, etc.)\n### Input parameters\n- The following parameters should be configured according to the data.\n    - Minimum number of cells expressed per gene. The workflow default is 3.\n    - Minimum number of genes expressed per cell. The workflow default is 200.\n    - Maximum number of genes expressed per cell. The workflow default is 2500.\n    - Size of the local neighborhood. Number of neighbours for computing neighborhood graph. The default is 15.\n    - Size of the local neighborhood (aka resolution) in louvain algorithm. The default is 1.\n\n## Processing\n\n- The workflow creates an **Anndata** object from the given input files.\n- Quality control performed. Cells are filtered by number of genes expressed, cells with high mitochondial content are removed.\n- Then counts are normlized and scaled\n- PCA is used for dimensionality reduction and 50 PCs are computed. Various plots are generated to inspect the PCA and PCA loadings that helps in chodeterminingnumber of  PCs to keep for further analysis.\n- Clustering is performed by computing a neighbourhood graph, and then using **louvain** algorithm. neighborhood graph is embeded into UMAP and plotted.\n- Marker genes are identified using **Wilcoxon rank sum test**. Marker genes expressions are visualized in various plots.\n- Optionally, louvain clusters can be annotated with cell types based on the marker genes.\n\n## Outputs\n\n- Final output is an Anndata object with annotations of louvain clusters.\n- Some informative plots from QC to the end results\n",
        "updated": "2024-10-09T00:00:00",
        "categories": [],
        "collections": [
            "Single Cell"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/pseudobulk-worflow-decoupler-edger/main",
        "iwcID": "pseudobulk-worflow-decoupler-edger-main",
        "readme": "# Single-Cell Pseudobulk Differential Expression Analysis with edgeR\n\nThis workflow uses the decoupler tool in Galaxy to generate pseudobulk counts from an annotated AnnData file obtained from scRNA-seq analysis. Following the pseudobulk step, differential expression genes (DEG) are calculated \nusing the edgeR tool. The workflow also includes data sanitation steps to ensure smooth operation of edgeR and minimizing potential issues. Additionally, a Volcano plot tool is used to visualize the results after the DEG \nanalysis.\n\nThe workflow deposited here is based on an earlier version of the [Persist-SEQ](https://persist-seq.org/)[^1] Pseudo-bulk scRNA-seq pipeline, of which the latest version is available [here](https://usegalaxy.eu/published/workflow?id=c3a11e1ac1aa8383). In terms of core procedures, the main differences with the IWC workflow are that the Persist-SEQ workflow:\n\n[^1]: The PERSIST-SEQ consortium is funded by the Innovative Medicines Initiative (IMI) Joint Undertaking, which receives support from the European Union's Horizon 2020 research and innovation program and EFPIA.\n\n- Is more opinionanted on the downstream enrichment analysis for the cancer biology use case.\n- Enables filtering out of seldomly expressed genes (with a configurable threshold) per contrast after DE calling, which reduces poorly supported highly DE genes and improves the signal for downstream enrichment analysis.\n\n## Inputs\n\n- deCoupler: Source AnnData (`h5ad`).\n    - Parameter: Pseudobulk: Fields to merge / optional \n    - Parameter: Group by column / has to be given\n    - Parameter: Sample key column / has to be given\n    - Parameter: Name your raw count layer / has to be given\n    - Parameter: Factor Field / has to be given\n- edgeR:\n    - Sanitzed Count Matrix\n    - Sanitized Factor File\n    - Cleaned Gene Annotations file\n    - Parameter: Formula for linear model / has to be given\n    - Contrast file / has to be given\n- Volcano Plot:\n    - Input (`tabular`) file with genesymbol, logFC, Pvalue and FDR columns.\n\n## Processing\n\nSanitzation steps after decoupler:\n- Sanitize Matrix and Factors(`tabular`): finds [ --+*^]+ and replace with -\n- Remove start, end with (`tabular`): A column that may affect EdgeR and DESeq2.\n- Sanitize First Factor for leading digits (`tabular`): Finds ^([0-9])(.+) and replace it with GG_\\\\1\\\\2\n- Get Contrast labels\n- Replace text\n- Split Contrasts\n- Contrasts as Parameters: Plot title\n- Select columns for volcano plot using (`Remove columns`) from DEG edgeR (`Table`)output.\n\n\n## Outputs\n\n  - Pseudobulk_count_matrix (`tabular`)\n  - Pseudobulk Plot (`png`)\n  - Filtered by expression (`png`)\n  - Table DEG\n  - Results (`HTML`) File and plots for download within the output as (`png`)\n  - Volcano plot (`PDF`)\n",
        "updated": "2024-11-18T00:00:00",
        "categories": [],
        "collections": [
            "Transcriptomics",
            "Single Cell"
        ]
    },
    {
        "name": "Velocyto-on10X-from-bundled",
        "trsID": "#workflow/github.com/iwc-workflows/velocyto/Velocyto-on10X-from-bundled",
        "iwcID": "velocyto-velocyto-on10x-from-bundled",
        "readme": "# RNA Velocity Analysis: Velocyto for 10X Genomics Data\n\nThese workflows simply run velocyto. There are 2 workflows because one can be easily run after the Single-Cell RNA-seq Preprocessing: 10X Genomics v3 to Seurat-Compatible Format' workflows (RNA Velocity Analysis: Velocyto for 10X Data from Bundled Output). The other can be easily run from uploaded datasets (RNA Velocity Analysis: Velocyto for 10X Data with Filtered Barcodes).\n\n## Input datasets\n\n- BAM files with CB and UB: A collection of BAM. It accepts BAM from cellranger or STARsolo with the CB and UB tags (if you use the fastq-to-matrix-10x workflows these tags are automatically included).\n- filtered barcodes (only for Velocyto_on10X_filtered_barcodes workflow): A collection of filtered barcodes (this is what will be used by velocyto). 'Filtered' means that these barcodes have been identified as potential cells. It should not be the whole list of 3 million possible barcodes from cellranger.\n- filtered matrices in bundle (only for Velocyto_on10X_from_bundled workflow): A collection of filtered matrices as bundled (like the one which comes from the fastq-to-matrix-10x workflows): A collection with as many items as samples. For each sample, the item is a list with 3 datasets (barcodes, genes, matrix). The workflow will then extract the items which have the 'barcodes' identifier.\n- gtf file: A file with annotations where exons are and how they are grouped into genes.\n\n## Processing\n\n- If you provided matrices, the first step is to extract barcodes.\n- For both cases velocyto cli is run to get a loom file per sample with spliced and unspliced counts.\n",
        "updated": "2024-02-05T00:00:00",
        "categories": [],
        "collections": [
            "Single Cell"
        ]
    },
    {
        "name": "Velocyto-on10X-filtered-barcodes",
        "trsID": "#workflow/github.com/iwc-workflows/velocyto/Velocyto-on10X-filtered-barcodes",
        "iwcID": "velocyto-velocyto-on10x-filtered-barcodes",
        "readme": "# RNA Velocity Analysis: Velocyto for 10X Genomics Data\n\nThese workflows simply run velocyto. There are 2 workflows because one can be easily run after the Single-Cell RNA-seq Preprocessing: 10X Genomics v3 to Seurat-Compatible Format' workflows (RNA Velocity Analysis: Velocyto for 10X Data from Bundled Output). The other can be easily run from uploaded datasets (RNA Velocity Analysis: Velocyto for 10X Data with Filtered Barcodes).\n\n## Input datasets\n\n- BAM files with CB and UB: A collection of BAM. It accepts BAM from cellranger or STARsolo with the CB and UB tags (if you use the fastq-to-matrix-10x workflows these tags are automatically included).\n- filtered barcodes (only for Velocyto_on10X_filtered_barcodes workflow): A collection of filtered barcodes (this is what will be used by velocyto). 'Filtered' means that these barcodes have been identified as potential cells. It should not be the whole list of 3 million possible barcodes from cellranger.\n- filtered matrices in bundle (only for Velocyto_on10X_from_bundled workflow): A collection of filtered matrices as bundled (like the one which comes from the fastq-to-matrix-10x workflows): A collection with as many items as samples. For each sample, the item is a list with 3 datasets (barcodes, genes, matrix). The workflow will then extract the items which have the 'barcodes' identifier.\n- gtf file: A file with annotations where exons are and how they are grouped into genes.\n\n## Processing\n\n- If you provided matrices, the first step is to extract barcodes.\n- For both cases velocyto cli is run to get a loom file per sample with spliced and unspliced counts.\n",
        "updated": "2024-02-05T00:00:00",
        "categories": [],
        "collections": [
            "Single Cell"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/lcms-preprocessing/main",
        "iwcID": "lcms-preprocessing-main",
        "readme": "# Mass spectrometry: LC-MS preprocessing with XCMS \n\nThis workflow uses the XCMS tool R package [(Smith, C.A. 2006)](https://bioconductor.org/packages/release/bioc/html/xcms.html) to extract, filter, align and fill gaps, and uses the CAMERA R package [(Kuhl, C 2012)](https://bioconductor.org/packages/release/bioc/html/CAMERA.html) to annotate isotopes, adducts and fragments.\n\n\ud83c\udf93 For more information see the [Galaxy Training Network tutorial: Mass spectrometry: LC-MS preprocessing with XCMS](https://training.galaxyproject.org/training-material/topics/metabolomics/tutorials/lcms-preprocessing/tutorial.html)\n\n## Inputs\n### sampleMetadata\nThe sampleMetadata tabular file corresponds to a table containing information about your samples\n\nA sample metadata file contains various information for each of your raw files:\n- Classes which will be used during the preprocessing steps\n- Analytical batches which will be useful for a batch correction step, along with sample types (pool/sample) and injection order\n- Different experimental conditions which can be used for statistics\n- Any information about samples that you want to keep, in a column format\n\nThe content of your sample metadata file has to be filled by you, since it is not contained in your raw data. Note that you can either:\n- Upload an existing metadata file\n- Use a template to create one (because it can be painful to get the sample list without misspelling or omission)\n  - Generate a template with the `xcms get a sampleMetadata file` tool available in Galaxy\n  - Fill it using your favorite table editor (Excel, LibreOffice)\n  - Upload it within Galaxy\n\n**Formats:** tab-separated values as tsv, tab, txt, ...\n\n### Mass-spectrometry Dataset Collection\nMass-spectrometry data files gathered in a Galaxy Dataser Collection\n\n**Formats:** open format as mzXML, mzMl, mzData and netCDF\n\n## Main steps\n1. MSnbase readMSData: read the mzXML and prepare for xcms\n2. XCMS findChromPeaks: peak picking\n3. XCMS groupChromPeaks: determining shared ions across samples\n4. XCMS adjustRtime: retention time correction\n5. XCMS fillChromPeaks: integrating areas of missing peaks\n6. CAMERA.annotate: annotation\n",
        "updated": "2025-05-05T00:00:00",
        "categories": [],
        "collections": [
            "Computational Chemistry",
            "Metabolomics"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/mfassignr/main",
        "iwcID": "mfassignr-main",
        "readme": "# Molecular Formula Assignment and Recalibration Workflow with MFAssignR\n\nThis workflow is designed for molecular formula assignment and recalibration of mass spectrometry data using the MFAssignR tool. It processes feature tables to generate recalibrated series, molecular formula assignments, and various diagnostic plots.\n\n## Workflow Steps\n\n1. **Input Feature Table**:\n   - Accepts a feature table in tabular format containing mass spectrometry data.\n\n2. **Molecular Formula Assignment**:\n   - Assigns molecular formulas to features based on mass-to-charge ratios and isotopic patterns.\n\n3. **Recalibration**:\n   - Recalibrates mass spectrometry data to improve accuracy.\n\n4. **Visualization**:\n   - Generates diagnostic plots, including:\n     - Signal-to-noise (SN) plots.\n     - Mass-to-charge (MZ) error plots.\n     - Van Krevelen (VK) diagrams.\n     - Molecular formula assignment plots.\n\n## Inputs\n\n- **Feature Table**: A tabular file containing mass spectrometry data. Example input: `mfassignr_input.txt`.\n\n## Outputs\n\n- **Recalibrated Series**:\n  - `recal_series.tabular`: Recalibrated data series.\n  - `final_series.tabular`: Final recalibrated series.\n\n- **Molecular Formula Assignments**:\n  - `Ambig.tabular`: Ambiguous assignments.\n  - `Unambig.tabular`: Unambiguous assignments.\n\n- **Diagnostic Plots**:\n  - Signal-to-noise plot: `SNplot.png`.\n  - Mass-to-charge error plot: `MZplot.png`.\n  - Van Krevelen diagrams and molecular formula assignment plots for CHO and other elements.\n\n## Usage\n\nThis workflow is designed to be run on the Galaxy platform. Users can upload their feature table, configure parameters, and execute the workflow to obtain recalibrated data, molecular formula assignments, and diagnostic plots.\n\n## References\n\n- [MFAssignR Documentation](https://github.com/your-repo/mfassignr)\n\n## License\n\nThis workflow is distributed under the MIT License. Please ensure proper attribution when using or modifying this workflow.",
        "updated": "2025-04-16T00:00:00",
        "categories": [],
        "collections": [
            "Metabolomics"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/gcms-metams/main",
        "iwcID": "gcms-metams-main",
        "readme": "# Mass spectrometry: GCMS with metaMS \n\nThis workflow uses the XCMS tool R package [(Smith, C.A. 2006)](https://bioconductor.org/packages/release/bioc/html/xcms.html) to extract, filter, align and fill gaps, and uses the CAMERA R package [(Kuhl, C 2012)](https://bioconductor.org/packages/release/bioc/html/CAMERA.html) to annotate isotopes, adducts and fragments.\n\nThis workflow is composed with the XCMS tool R package [(Smith, C.A. 2006)](https://bioconductor.org/packages/release/bioc/html/xcms.html) to extract and the metaMS R package [(Wehrens, R 2014)](https://bioconductor.org/packages/release/bioc/html/metaMS.html) for the field of untargeted metabolomics. \n\n\ud83c\udf93 For more information see the [Galaxy Training Network tutorial: Mass spectrometry: GC-MS analysis with metaMS package](https://training.galaxyproject.org/training-material/topics/metabolomics/tutorials/gcms/tutorial.html)\n\n## Inputs\n### sampleMetadata\nThe sampleMetadata tabular file corresponds to a table containing information about your samples\n\nA sample metadata file contains various information for each of your raw files:\n- Classes which will be used during the preprocessing steps\n- Analytical batches which will be useful for a batch correction step, along with sample types (pool/sample) and injection order\n- Different experimental conditions which can be used for statistics\n- Any information about samples that you want to keep, in a column format\n\nThe content of your sample metadata file has to be filled by you, since it is not contained in your raw data. Note that you can either:\n- Upload an existing metadata file\n- Use a template to create one (because it can be painful to get the sample list without misspelling or omission)\n  - Generate a template with the `xcms get a sampleMetadata file` tool available in Galaxy\n  - Fill it using your favorite table editor (Excel, LibreOffice)\n  - Upload it within Galaxy\n\n**Formats:** tab-separated values as tsv, tab, txt, ...\n\n### Mass-spectrometry Dataset Collection\nMass-spectrometry data files gathered in a Galaxy Dataser Collection\n\n**Formats:** open format as mzXML, mzMl, mzData and netCDF\n\n## Main steps\n1. MSnbase readMSData: read the mzXML and prepare for xcms\n2. XCMS findChromPeaks: peak picking\n3. metaMS.runGC: definition of pseudo-spectra\n",
        "updated": "2025-05-05T00:00:00",
        "categories": [],
        "collections": [
            "Computational Chemistry",
            "Metabolomics"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/qcxms-sdf/main",
        "iwcID": "qcxms-sdf-main",
        "readme": "# QCxMS Spectra Prediction from SDF Workflow\n\nThis workflow predicts electron ionization (EI) mass spectra using QCxMS, starting from a single SDF file containing the 3D coordinates of all atoms in the molecule. These files can typically be obtained from PubChem. The workflow converts the input file, performs neutral and production runs, and generates predicted spectra in MSP format.\n\n## Workflow Steps\n\n1. **Input SDF File**:\n   - Accepts an SDF file containing one or multiple molecular structures with pre-generated conformers.\n\n2. **Conversion to XYZ Format**:\n   - Converts the input SDF file to XYZ format using Open Babel.\n\n3. **QCxMS Neutral Run**:\n   - Performs a neutral run to prepare the molecular structure for production calculations.\n\n4. **QCxMS Production Run**:\n   - Executes the production run to simulate fragmentation and generate intermediate results.\n\n5. **QCxMS Get Results**:\n   - Processes the results from the production run and generates the predicted EI mass spectra in MSP format.\n\n## Inputs\n\n- **Input SDF File**: A file containing molecular structures with 3D coordinates (e.g., obtained from PubChem).\n\n## Outputs\n\n- **Predicted Spectra**:\n  - An MSP file containing the predicted EI mass spectra for the input molecules.\n\n## Usage\n\nThis workflow is designed to be run on the Galaxy platform. Users can upload their SDF file, configure parameters, and execute the workflow to obtain predicted EI mass spectra.\n\n## References\n\n- [QCxMS Documentation](https://github.com/recetox/qcxms)\n- [Open Babel Documentation](http://openbabel.org/)\n\n## License\n\nThis workflow is distributed under the MIT License. Please ensure proper attribution when using or modifying this workflow.",
        "updated": "2025-04-28T00:00:00",
        "categories": [],
        "collections": [
            "Computational Chemistry",
            "Metabolomics"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/repeatmasking/main",
        "iwcID": "repeatmasking-main",
        "readme": "# RepeatMasking Workflow\n\nThis workflow uses RepeatModeler and RepeatMasker for genome analysis.\n\n- RepeatModeler is a software package for identifying and modeling de novo families of transposable elements (TEs). At the heart of RepeatModeler are three de novo repeat search programs (RECON, RepeatScout and LtrHarvest/Ltr_retriever) which use complementary computational methods to identify repeat element boundaries and family relationships from sequence data.\n\n- RepeatMasker is a program that analyzes DNA sequences for *interleaved repeats* and *low-complexity* DNA sequences. The result of the program is a detailed annotation of the repeats present in the query sequence, as well as a modified version of the query sequence in which all annotated repeats are present.\n\n## Input dataset for RepeatModeler\n- RepeatModeler requires a single input file, a genome in fasta format.\n\n\n## Outputs dataset for RepeatModeler\n- Two output files are generated:\n    - summary file (.tbl)\n    - fasta file containing alignments in order of appearance in the query sequence\n\n\n## Input dataset for RepeatMasker\n- ReapatMasker requires the fasta file generated by RepeatModeler\n\n## Outputs datasets for RepeatMasker\n- Five output files are generated:\n    - a fasta file\n    - .gff3 file\n    - a table summarizing the repeated content of the sequence analyzed\n    - a file with statistics related to the repeated content of the sequence analyzed\n    - a summary of the mutation sites found and the order of grouping\n    \n",
        "updated": "2023-09-21T00:00:00",
        "categories": [],
        "collections": [
            "Genome Annotation"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/allele-based-pathogen-identification/main",
        "iwcID": "allele-based-pathogen-identification-main",
        "readme": "# Allele-based Pathogen Identification\n\nThis workflow identifies pathogens using an allelic approach, detecting Single Nucleotide Polymorphisms (SNPs) to track emerging variants, i.e. markers showing evolutionary histories of homogeneous strains. This process includes SNP calling, aimed at identifying novel pathogen strains and elucidating discrepancies compared to reference sequences, thereby facilitating the tracking of emerging variants. Within this workflow, both variants and SNPs are discerned, serving as crucial elements for subsequent pathogen identification and variant tracking purposes.\n\n## Input Datasets\n- Collection of Pre-Processed Sequenced reads of all samples, ready for further analysis with the other workflows, in a `fastqsanger or fastqsanger.gz` format, the output of **Nanopore Preprocessing** workflow.\n- A reference genome to the tested pathogen.\n\n## Output Datasets\n- VCF files indicating identified variants and SNPs, BAM files with mapping results, and Tabular files with mapping depth and coverage calculations.\n\nIf you're unsure how to use this workflows, or if you want to see it in action with test datasets, it is included in our detailed training material for foodborne pathogen detection and tracking. You can find step-by-step instructions and practical examples in the following [GTN tutorial](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/pathogen-detection-from-nanopore-foodborne-data/tutorial.html)\n",
        "updated": "2025-03-10T00:00:00",
        "categories": [],
        "collections": [
            "Microbiome"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/gene-based-pathogen-identification/main",
        "iwcID": "gene-based-pathogen-identification-main",
        "readme": "# Gene-based Pathogen Identification\n\nIn this workflow, we determine whether the samples are pathogenic or not, by looking for genes known to be linked to pathogenicity or to the pathogenecity character of the organism.\n\n- Virulence Factor (VF): gene products, usually proteins, involved in pathogenicity. By identifying them, we can call a pathogen and its severity level\n\n- Antimicrobial Resistance genes (AMR).\n\n    These type of genes have three fundamental mechanisms of antimicrobial resistance that are enzymatic degradation of antibacterial drugs, alteration of bacterial proteins that are antimicrobial targets, and changes in membrane permeability to antibiotics, which will lead to not altering the target site and spread throughput the pathogenic bacteria decreasing the overall fitness of the host.\n\nIn this workflow we:\n\n1. Perform genome assembly to get contigs, i.e. longer sequences, using metaflye (Kolmogorov et al. 2020) then assembly polishing using medaka consensus pipeline and visualizing the assembly graph using Bandage Image (Wick et al. 2015)\n2. Generate reports with AMR genes and VF using ABRicate\n\n## Input Datasets\n- Collection of Pre-Processed Sequenced reads of all samples, ready for further analysis with the other workflows, in a `fastqsanger` or `fastqsanger.gz` format, the output of **Nanopore Preprocessing** workflow.\n\n## Output Datasets\n- FASTA and Tabular files to track genes and visualise our pathogenic identification through out all samples.\n\nIf you're unsure how to use this workflows, or if you want to see it in action with test datasets, it is included in our detailed training material for foodborne pathogen detection and tracking. You can find step-by-step instructions and practical examples in the following [GTN tutorial](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/pathogen-detection-from-nanopore-foodborne-data/tutorial.html)\n",
        "updated": "2024-04-18T00:00:00",
        "categories": [],
        "collections": [
            "Microbiome"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/nanopore-pre-processing/main",
        "iwcID": "nanopore-pre-processing-main",
        "readme": "# Nanopore Preprocessing\n\nBefore starting any analysis, it is always a good idea to assess the quality of your input data and to discard poor-quality base content by trimming and filtering reads.\n\nGenerally, we are not interested in the host sequences, but rather only those originating from the pathogen itself. It is important to get rid of all host sequences and to only retain sequences that might include a pathogen, both in order to speed up further steps and to avoid host sequences compromising the analysis.\n\n## Input Datasets\n\n- Collection of sequenced Nanopore reads of all samples to be analysed in a `fastqsanger` or `fastqsanger.gz` format.\n\n## Output Datasets\n\n- Collection of Pre-Processed Sequenced reads of all samples, ready for further analysis with the other workflows, in a `fastqsanger` or `fastqsanger.gz` format.\n\n- Tables indicating total number of reads before and after host sequences trimming, and the host sequences percentages found in each sample.\n\nIf you're unsure how to use this workflows, or if you want to see it in action with test datasets, it is included in our detailed training material for foodborne pathogen detection and tracking. You can find step-by-step instructions and practical examples in the following [GTN tutorial](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/pathogen-detection-from-nanopore-foodborne-data/tutorial.html)\n",
        "updated": "2024-04-25T00:00:00",
        "categories": [],
        "collections": [
            "Microbiome"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/taxonomy-profiling-and-visualization-with-krona/main",
        "iwcID": "taxonomy-profiling-and-visualization-with-krona-main",
        "readme": "# Taxonomy Profiling and Visualisation with Krona\n\nIn this workflow, we identify the different organisms found in our samples by assigning taxonomy levels to the reads starting from the kingdom level down to the species level and visualise the result.\n\nIt\u2019s important to check what might be the species of a possible pathogen to be found, it gets us closer to the investigation as well as discovering possible multiple pathogenetic infections if any existed.\n\nFor taxonomy profiling Kraken2 tool is used along with one of its standard databases available on Galaxy, you can freely choose between Kraken2 different databases based on your input datasets. For visualisation multiple tools can be used, Krona pie chart (as default in this workflow), Phinch interactive tool, Pavian, etc.\n\n## Input Datasets\n- Collection of Pre-Processed Sequenced reads of all samples, ready for further analysis with the other workflows, in a `fastqsanger` or `fastqsanger.gz` format, the output of **Nanopore Preprocessing** workflow.\n\n## Output Datasets\n- Taxonomy profiling Tabular file, visualisation figures and interactive pie charts.\n\nIf you're unsure how to use this workflows, or if you want to see it in action with test datasets, it is included in our detailed training material for foodborne pathogen detection and tracking. You can find step-by-step instructions and practical examples in the following [GTN tutorial](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/pathogen-detection-from-nanopore-foodborne-data/tutorial.html)\n",
        "updated": "2024-04-25T00:00:00",
        "categories": [],
        "collections": [
            "Microbiome"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/pathogen-detection-pathogfair-samples-aggregation-and-visualisation/main",
        "iwcID": "pathogen-detection-pathogfair-samples-aggregation-and-visualisation-main",
        "readme": "# Pathogen Detection: PathoGFAIR Samples Aggregation and Visualisation\n\nIn this workflow, we will aggregate results and use the results from 3 workflows (**Nanopore Preprocessing**, **Gene-based Pathogen Identification** and **Nanopore Allele-based Pathogen Identification**) to help track pathogens among samples and visualise all performed analysis by:\n\n1. Drawing a presence-absence heatmap of the identified VF genes within all samples to visualise in which samples these genes can be found.\n2. Drawing a phylogenetic tree for each pathogenic genes detected, where we will relate the contigs of the samples together where this gene is found.\n3. Plotting QC reads, host reads, mapping coverage and depth, and SNP analysis.\n\nWith these types of visualisations, we can have an overview of all samples and the genes, but also how samples are related to each other, which common pathogenic genes they share. Given the time of the sampling and the location one can easily identify using these graphs, where and when the contamination has occurred among the different samples.\n\nIf you're unsure how to use this workflows, or if you want to see it in action with test datasets, it is included in our detailed training material for foodborne pathogen detection and tracking. You can find step-by-step instructions and practical examples in the following [GTN tutorial](https://training.galaxyproject.org/training-material/topics/microbiome/tutorials/pathogen-detection-from-nanopore-foodborne-data/tutorial.html)\n",
        "updated": "2024-04-24T00:00:00",
        "categories": [],
        "collections": [
            "Microbiome"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/mags-building/main",
        "iwcID": "mags-building-main",
        "readme": "# Metagenome-Assembled Genomes (MAGs) Generation  \n\nThis workflow generates Metagenome-Assembled Genomes (MAGs) from paired short reads.  \nDereplicated MAGs for the complete input sample set are reported.\n\n## Workflow Logic  \n\nThe workflow supports assembly using **metaSPADES** and **MEGAHIT**.  \nFor binning, it utilizes four different tools: **MetaBAT2, MaxBin2, SemiBin, and CONCOCT**. The resulting bins are then refined using **Binette**, the successor of metaWRAP.  \n\n## MAGs Annotation and Quality Control  \n\nAfter binning, the resulting MAGs are **dereplicated** across all input samples based on **CheckM2 quality metrics** using **dRep**. The following processing steps are then performed:  \n\n- **Annotation** with Bakta  \n- **Taxonomic Assignment** using GTDB-Tk  \n- **Quality Control** via QUAST and CheckM/CheckM2  \n- **Abundance Estimation** per sample with CoverM  \n\nAll results are consolidated into a single **MultiQC report** for easy analysis.  \n\n## Input Requirements  \n\nInput reads must be quality-filtered, with host reads removed. \n\n- **Trimmed reads**: Quality-trimmed reads from individual samples, used solely for abundance estimation.  \n- **Trimmed reads from grouped samples**: These reads need to be grouped based on the desired MAGs generation approach:  \n  - **Individual MAGs Generation**: Use the same input as `Sample-wise Trimmed Paired Reads` to generate MAGs per sample.  \n  - **Pooled MAGs Generation (Co-assembly/Binning)**: Merge all reads input one file for a fully pooled MAGs approach.  \n  - **Grouped MAGs Generation (Co-assembly/Binning)**: Merge samples based on predefined groups.  \n  - **Hybrid MAGs Generation**: Combine individual and grouped reads for a mixed approach.  \n\n> **Note**: Merging reads can result in large input files, significantly increasing computational demands\u2014especially during assembly and binning, which may require substantial RAM. Our tests with synthetic samples up to **50 GB** showed feasible performance. For larger datasets, we recommend limiting the approach to **individual or pooled MAGs generation**.  \n",
        "updated": "2025-05-19T00:00:00",
        "categories": [],
        "collections": [
            "Microbiome"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/rnaseq-de/main",
        "iwcID": "rnaseq-de-main",
        "readme": "# RNA-Seq Differential Expression Analysis with Visualization\n\nThis workflow works only with an experimental setup containing exactly 2 conditions with at least 2 replicates per condition.\n\n## Inputs dataset\n\n- Counts from changed condition: Counts from experimental condition or changed condition. For eg. counts from treatment or knockdown samples.\n- Counts from reference condition: Counts from reference condition or base condition. For eg. counts from untreated or wildtype samples.\n- Gene Annotaton: The same GTF file used for mapping and quantification. It is used to annotate the DESeq2 results table. Ideally, the GTF file should contain `gene_id`, `gene_biotype` and `gene_name` attributes.\n\n## Inputs values\n\n- Count files have header: Indicate whether your input count files have a header line. Usually, count files generated from featureCounts tool have a header line whereas count files from RNA-STAR do not have.\n- Adjusted p-value threshold: All the genes with an adjusted p-value less than this value are considered as differentially expressed. With a value of 0.05, expect 5% of false positives in the differentially expressed genes list. If empty, a default value of 0.05 is used.\n- log2 fold change threshold: All the genes with an absolute fold change (regarless of up or down regulation) more than this value are selected. A log2 FC of 3 equals to an absolute fold change of 8 (2\u00b3). If empty, a default value of 1.0 is used.\n\n## Processing\n\n- The workflow uses DESeq2 for performing differential expression analysis. In addition to the results table, it also produces normalized counts table.\n- The results table is annotated with gene positions, biotypes, gene symbols.\n- The annotated results table is further filtered with the input adjusted p-value and log2 fold change thresholds.\n- A valcano plot is generated with top 10 significantly differentially expressed genes.\n- A heatmap of log trasformed normalized counts and another heatmap of Z-scores is generated.\n",
        "updated": "2025-03-24T00:00:00",
        "categories": [],
        "collections": [
            "Transcriptomics"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/rnaseq-sr/main",
        "iwcID": "rnaseq-sr-main",
        "readme": "# RNA-Seq Analysis: Single-End Read Processing and Quantification\n\n## Inputs dataset\n\n- Collection of FASTQ files: The workflow needs a list of datasets of fastqsanger.\n- GTF file of annotation: A gtf file with genes annotation.\n- GTF with regions to exclude from FPKM normalization with Cufflinks: Optional, but recommended. A gtf file with regions to exclude from normalization in Cufflinks.\n\n  - For instance a gtf that masks chrM for the mm10 genome:\n\n```\nchrM\tchrM_gene\texon\t0\t16299\t.\t+\t.\tgene_id \"chrM_gene_plus\"; transcript_id \"chrM_tx_plus\"; exon_id \"chrM_ex_plus\";\nchrM\tchrM_gene\texon\t0\t16299\t.\t-\t.\tgene_id \"chrM_gene_minus\"; transcript_id \"chrM_tx_minus\"; exon_id \"chrM_ex_minus\";\n```\n\n## Inputs values\n\n- Forward adapter (optional): If not provided, fastp will try to guess the adapter sequence from the data. Its sequences  depends on the library preparation. Usually classical Illumina RNA libraries are Truseq and ISML (relatively new Illumina library) is Nextera. If you don't know, use FastQC to determine if it is Truseq or Nextera. If the read length is relatively short (50bp), there is probably no adapter so it will not impact your results.\n- Generate additional QC reports: whether to compute additional QC: FastQC, Picard, Read distribution on genomic features, gene body coverage, reads per chromosomes.\n- Reference genome: this field will be adapted to the genomes available for STAR.\n- Strandedness: For stranded RNA, reverse means that the read is complementary to the coding sequence, forward means that the read is in the same orientation as the coding sequence. This will only count alignments that are compatible with your library preparation strategy. This is also used for the stranded coverage and for FPKM computation with cufflinks/StringTie.\n- Use featureCounts for generating count tables: Whether to use count tables from featureCounts instead of from STAR.\n- Compute Cufflinks FPKM: Whether you want to get FPKM with Cufflinks (pretty long).\n- Compute StringTie FPKM: Whether you want to get FPKM/TPM etc... with StringTie.\n\n## Processing\n\n- The workflow will remove adapters and low quality bases and filter out any read smaller than 15bp.\n- The filtered reads are mapped with STAR with ENCODE parameters (for long RNA-seq but I use it for short also). STAR is also used to count reads per gene and generate strand-specific normalized coverage (on uniquely mapped reads).\n- A multiQC is run to have an overview of the QC. This can also be used to get the strandedness.\n- FPKM values for genes and transcripts are computed with cufflinks using correction for multi-mapped reads (this step is optionnal).\n- FPKM/TPM values for genes are computed with StringTie (this step is optional).\n- The BAM is filtered to keep only uniquely mapped reads (tag NH:i:1).\n- Unstranded coverage is computed with bedtools and normalized to the number of million uniquely mapped reads.\n- The three coverage files are converted to bigwig.\n\n### Warning\n\n- The coverage stranded output depends on the strandedness of the library:\n  - If you have an unstranded library, stranded coverages are useless\n  - If you have a forward stranded library, the label matches the orientation of reads.\n  - If you have a reverse stranded library, `forward` should correspond to genes on the forward strand and uses the reads mapped on the reverse strand. `reverse` should correspond to genes on the reverse strand and uses the reads mapped on the forward strand.\n\n## Contribution\n\n### Version 0.1\n\n@lldelisle wrote the workflow and the tests.\n\n@nagoue updated the tools, made it work in usegalaxy.org, fixed some best practices.\n\n### Version 1.0\n\n@pavanvidem added the new features (featurecount + additional QC) and found a smaller test dataset.\n",
        "updated": "2025-01-27T00:00:00",
        "categories": [],
        "collections": [
            "Transcriptomics"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/brew3r/main",
        "iwcID": "brew3r-main",
        "readme": "# BREW3R\n\n![BREW3R logo](https://raw.githubusercontent.com/lldelisle/BREW3R/main/images/logo.png)\n\nBREW3R stands for **B**ulk **R**NA-seq **E**vidence-based **W**orkflow for **3**' UTR **R**eannotation. \n\nThis workflow enables extending an existing gtf downloaded on a public website, like Ensembl, Genecode or UCSC, using *de novo* gene annotation with StringTie on full length bulk RNA-seq.\n\nBREW3R highly relies on a R package called BREW3R.r available on [bioconductor](https://bioconductor.org/packages/release/bioc/html/BREW3R.r.html).\n\n## Input datasets\n\n- The workflow requires an input gtf file which will be extended.\n- As well as a collection of BAM files.\n\n## Input values\n\n- strandedness: Must be one of `stranded - forward`, `stranded - reverse` and `unstranded`. For stranded libraries, reverse means that the read is complementary to the coding sequence, forward means that the read is in the same orientation as the coding sequence.\n- minimum coverage: Minimum reads per bp coverage to consider for assembly in each de novo assembly (for each BAM file). Default: 10\n- minimum FPKM for merge: Minimum FPKM value for a transcript to be included into the merged de novo assembly.\n\n## Processing\n\n- StringTie is called once per input BAM file to compute de novo assembly.\n- StringTie is called to merge all outputs of previous steps.\n- BREW3R.r is run with the default parameters on the input gtf to extend and the output of StringTie. If the library was unstranded all merged transcripts without orientation that overlaps exons of both strands are not used for the extension.\n",
        "updated": "2024-10-07T00:00:00",
        "categories": [],
        "collections": [
            "Transcriptomics"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/goseq/main",
        "iwcID": "goseq-main",
        "readme": "# Gene Ontology and KEGG Pathway Enrichment Analysis\n\n## Inputs dataset\n\nThe workflow need the following inputs:\n- **The DEG file:**\n    - A tabular file with first column the gene symbol and second column a boolean value whether the gene is a differentially expressed gene or not. \n- **The gene length file:**\n    - A tabular file with first column the gene symbol and second column the gene length of the genes.\n    - You can create this file with **[Gene length and GC content](https://usegalaxy.eu/?tool_id=toolshed.g2.bx.psu.edu%2Frepos%2Fiuc%2Flength_and_gc_content%2Flength_and_gc_content%2F0.1.2&version=latest)** tool. You will need a GTF file as input.\n    - If you are using **[featureCounts](https://usegalaxy.eu/?tool_id=toolshed.g2.bx.psu.edu%2Frepos%2Fiuc%2Ffeaturecounts%2Ffeaturecounts%2F2.0.6%2Bgalaxy0&version=latest)** you can set `Create gene-length file` to yes and get gene length as separate output.\n- **The KEGG file:**\n    - A tabular file with first column the Pathway ID and second column the Pathway name like: \n        -   ```\n            ID  Name\n            01100   Metabolic pathways - mmus\n            01200   Carbon metabolism - mmus \n            ```\n    - You can get this information from the KEGG database. For example:\n        - mouse: https://rest.kegg.jp/list/pathway/mmu\n        - human: https://rest.kegg.jp/list/pathway/hsa\n- **Genome:** Select one of the available genomes\n- **Gene ID format:** Select the format of your input genes (Ensembl, Entrez, or Symbol)\n## Processing\n\n- The workflow will do a simple enrichment analysis with taking into account the gene length\n- The output will be 3 files `GO table`, `Top ontology plot` and `DE genes in each category` for Cellular Component, Biological Processes, and Molecular Function ontologies and `KEGG table` and `DE genes in each KEGG Pathways`\n\n## Contribution\n\n@nilchia wrote the workflow and the tests.",
        "updated": "2024-11-03T00:00:00",
        "categories": [],
        "collections": [
            "Transcriptomics"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/rnaseq-pe/main",
        "iwcID": "rnaseq-pe-main",
        "readme": "# RNA-Seq Analysis: Paired-End Read Processing and Quantification\n\n## Inputs dataset\n\n- Collection paired FASTQ files: The workflow needs a list of dataset pairs of fastqsanger.\n- GTF file of annotation: A gtf file with genes annotation.\n- GTF with regions to exclude from FPKM normalization with Cufflinks: Optional, but recommended. A gtf file with regions to exclude from normalization in Cufflinks.\n\n  - For instance a gtf that masks chrM for the mm10 genome:\n\n```\nchrM\tchrM_gene\texon\t0\t16299\t.\t+\t.\tgene_id \"chrM_gene_plus\"; transcript_id \"chrM_tx_plus\"; exon_id \"chrM_ex_plus\";\nchrM\tchrM_gene\texon\t0\t16299\t.\t-\t.\tgene_id \"chrM_gene_minus\"; transcript_id \"chrM_tx_minus\"; exon_id \"chrM_ex_minus\";\n```\n\n## Inputs values\n\n- Forward and Reverse adapter (optional): By default, fastp will try to overlap both reads and will only use these sequences if R1/R2 are found not overlapped. Their sequences depends on the library preparation. Usually classical Illumina RNA libraries is Truseq and ISML (relatively new Illumina library) is Nextera.\n- Generate additional QC reports: whether to compute additional QC: FastQC, Picard, Read distribution on genomic features, gene body coverage, reads per chromosomes.\n- Reference genome: this field will be adapted to the genomes available for STAR.\n- Strandedness: For stranded RNA, reverse means that the first read in a pair is complementary to the coding sequence, forward means that the first read in a pair is in the same orientation as the coding sequence. This will only count alignments that are compatible with your library preparation strategy. This is also used for the stranded coverage and for FPKM computation with cufflinks/StringTie.\n- Use featureCounts for generating count tables: Whether to use count tables from featureCounts instead of from STAR.\n- Compute Cufflinks FPKM: Whether you want to get FPKM with Cufflinks (pretty long).\n- Compute StringTie FPKM: Whether you want to get FPKM/TPM etc... with StringTie.\n\n## Processing\n\n- The workflow will remove adapters and low quality bases and filter out any read smaller than 15bp.\n- The filtered reads are mapped with STAR with ENCODE parameters (for long RNA-seq but I use it for short also). STAR is also used to count reads per gene and generate strand-specific normalized coverage (on uniquely mapped reads).\n- Optionally featureCounts is used to generate count files when this option enabled.\n- Optionally FastQC, Picard, read_distribution, geneBody_coverage, samtools idxstats, Picard are run to get additional QC.\n- A multiQC is run to have an overview of the QC. This can also be used to get the strandedness.\n- FPKM values for genes and transcripts are computed with cufflinks using correction for multi-mapped reads (this step is optionnal).\n- FPKM/TPM values for genes are computed with StringTie (this step is optional).\n- The BAM is filtered to keep only uniquely mapped reads (tag NH:i:1).\n- Unstranded coverage is computed with bedtools and normalized to the number of million uniquely mapped reads.\n- The three coverage files are converted to bigwig.\n\n### Warning\n\n- The coverage stranded output depends on the strandedness of the library:\n  - If you have an unstranded library, stranded coverages are useless\n  - If you have a forward stranded library, the label matches the orientation of the first read in pairs.\n  - If you have a reverse stranded library, the label matches the orientation of the second read in pairs.\n",
        "updated": "2025-01-27T00:00:00",
        "categories": [],
        "collections": [
            "Transcriptomics"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/annotation-helixer/main",
        "iwcID": "annotation-helixer-main",
        "readme": "# Genome annotation workflow with Helixer\n\nThis workflow allows you to annotate a genome with Helixer and evaluate the quality of the annotation using BUSCO and Genome Annotation statistics. GFFRead is also used to predict protein sequences derived from this annotation, and BUSCO and OMArk are used to assess proteome quality. \n\n\nHelixer is an annotation software with a new and different approach: it performs evidence-free predictions (no need for RNASeq data or sequence aligments), using Graphics Processing Unit (GPU), with a much faster execution time. The annotation is based on the development and use of a cross-species deep learning model. The software is used to configure and train models for ab initio prediction of gene structure. In other words, it identifies the base pairs in a genome that belong to the UTR/CDS/Intron genes.\n\nTo assess the quality of the proteome, we will use the GFFRead tool to extract the predicted protein sequences from the annotation (i.e. the Helixer annotation).\n\nTo assess the quality of the annotation, we will use different tools:\n- Genome Annotation Statistics: is a program designed to analyze and provide statistics on genomic annotations. This software performs its analyses from a GFF3 file.\n- BUSCO (Benchmarking Universal Single-Copy Orthologs):  is a tool allowing to evaluate the quality of a genome assembly or of a genome annotation. By comparing genomes from various more or less related species, the authors determined sets of ortholog genes that are present in single copy in (almost) all the species of a clade (Bacteria, Fungi, Plants, Insects, Mammals, \u2026). Most of these genes are essential for the organism to live, and are expected to be found in any newly sequenced and annotated genome from the corresponding clade. Using this data, BUSCO is able to evaluate the proportion of these essential genes (also named BUSCOs) found in a set of (predicted) transcript or protein sequences. This is a good evaluation of the \u201ccompleteness\u201d of the annotation.\n- OMArk: is proteome quality assessment software. It provides measures of proteome completeness, characterises the consistency of all protein-coding genes with their homologues and identifies the presence of contamination by other species. OMArk is based on the OMA orthology database, from which it exploits orthology relationships, and on the OMAmer software for rapid placement of all proteins in gene families.\n\nThe final step is to view the generated annotation using a genome browser such as JBrowse. This browser allows you to navigate along the chromosomes of the genome and view the structure of each predicted gene.\n\n## Input dataset for Helixer\nHelixer requires the genome sequence to be annotated, in fasta format.\n\n## Output dataset for Helixer\nHelixer produces a single output dataset: a GFF3 file. \n\n\n## Input dataset for Genome Annotation Statistics\nThis software requires a GFF3 file. In this workflow, the output generated is Helixer.\n\n## Output dataset for Genome Annotation Statistics\nTwo output files are generated:\n- a file containing graphs in pdf format\n- a summary in txt format\n\n## Input dataset for GFFRead\nIn this workflow, GFFRead requires two inputs:\n- an annotation file in GFF3 format (the Helixer format)\n- the genome sequence in fasta format\n\n## Output dataset for GFFRead\nIn this workflow, a unique output will be generated. This file, in fasta format, contains the protein sequences predicted from the annotation.\n\n## Input dataset for BUSCO\nBUSCO requires a fasta file.\nBUSCO will be used twice for this workflow. Firstly on the predicted protein sequences and secondly on the genome sequence. \n\n## Output dataset for BUSCO\nWith BUSCO, we can obtain different output files:\n- short summary : statistical summary of the quality of genomic assembly or annotation, including total number of genes evaluated, percentage of complete genes, percentage of partial genes, etc.\n- full table : list of universal orthologs found in the assembled or annotated genome, with information on their completeness, location in the genome, quality score, etc.\n- missing BUSCOs : list of orthologs not found in the genome, which may indicate gaps in assembly or annotation.\n- summary image : graphics and visualizations to visually represent the results of the evaluation, such as bar charts showing the proportion of complete, partial and missing genes.\n- GFF : contain information on gene locations, exons, introns, etc.\n\n## Input dataset for OMArk\nOMAk requires the fasta file produced by GFFRead, containing the predicted protein sequences. \n\n## Output dataset for OMArk\nIn this tutorial, a single output file will be generated: a file detailing the assessment of completeness, consistency and species composition. \n\n## Input dataset for JBrowse\nJBrowse requires two inputs:\n- the genome sequence in fasta format\n- the annotation file in gff3 format, generated by Helixer\n\n## Output dataset for JBrowse\nAn html file is generated for browsing the genome.",
        "updated": "2025-01-27T00:00:00",
        "categories": [],
        "collections": [
            "Genome Annotation"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/annotation-maker/main",
        "iwcID": "annotation-maker-main",
        "readme": "# Genome annotation workflow with Maker\n\nThis workflow allows for genome annotation using Maker and evaluates the quality of the annotation with BUSCO and genome annotation statistics. The annotation can then be improved, standardized, and visualized with additional tools.\n\n**Maker** is a genome model prediction software that uses ab initio predictors (SANP and Augustus) to improve its predictions. Maker is capable of annotating both prokaryotes and eukaryotes. It works by aligning as much evidence as possible along the genome sequence, then reconciling all these signals to determine likely genetic structures.\n\n## Workflow Steps\n\n- Annotation with Maker: Maker uses the genome sequence, protein evidence, ab-initio predictions, and ESTs to produce the annotation.\n- Quality Evaluation:\n    - Run Fasta Statistics to assess genome assembly quality.\n    - Use BUSCO to evaluate annotation completeness.\n- Annotation Statistics: Analyze the annotation using Genome Annotation Statistics, producing graphical and textual summaries.\n- Sequence Extraction: Extract predicted protein sequences using GFFRead for downstream analysis.\n- Improve Gene Names: Standardize gene names using Map annotation ids for better readability.\n- Visualization: Load the genome sequence and annotation into JBrowse for interactive browsing.\n\n## Input data\nThe following input files are required for the workflow:\n- Genome sequence (FASTA format): The genome to be annotated. Used by Maker, Fasta Statistics, and BUSCO.\n- Protein sequences (FASTA format): Evidence to assist annotation in Maker.\n- EST evidences (FASTA format): Alignments used as evidence by Maker.\n- Ab-initio gene predictions: Supplementary data for Maker to refine annotations.\n\n\n## Output Data\nThe workflow generates the following outputs:\n- Annotation file (GFF3): Contains the final consensus gene models produced by Maker.\n- Genome statistics: A tabular file summarizing contig sizes and base content, produced by Fasta Statistics.\n- BUSCO results: Assess the completeness of the annotation and include:\n    - A summary of results.\n    - A table of all searched BUSCO genes with their status.\n    - A table of missing BUSCO genes.\n- Annotation statistics: Summary and graphical analyses of the annotation, produced by Genome Annotation Statistics.\n- Protein sequences (FASTA): Predicted from the annotation using GFFRead.\n- Renamed GFF annotation file: Contains standardized gene names, produced by Map annotation ids.\n- Genome browser visualization (HTML): An interactive genome view produced by JBrowse.",
        "updated": "2025-03-06T00:00:00",
        "categories": [],
        "collections": []
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/functional-annotation-protein-sequences/main",
        "iwcID": "functional-annotation-protein-sequences-main",
        "readme": "# Functional annotation of protein sequences Workflow\n\nThis workflow uses eggNOG mapper and Interproscan for functional annotation of protein sequences.\nIt can be used on proteins from any organism.\n\nEggNOG Mapper compares each protein sequence of the annotation to a huge set of ortholog groups from the EggNOG database. In this database, each ortholog group is associated with functional annotation like Gene Ontology (GO) terms or KEGG pathways. When the protein sequence of a new gene is found to be very similar to one of these ortholog groups, the corresponding functional annotation is transfered to this new gene.\n\nInterProScan is a tool that analyses each protein sequence from our annotation to determine if they contain one or several of the signatures from InterPro. When a protein contains a known signature, the corresponding functional annotation will be assigned to it by InterProScan.\n\n## Input dataset\nThis workflow requires only a input file: a protein sequences file in fasta format.\n\n\n## Outputs for eggNOG Mapper\nThe output of this tool is a tabular file, where each line represents a gene from our annotation, with the functional annotation that was found by EggNOG-mapper. It includes a predicted protein name, GO terms, EC numbers, KEGG identifiers, etc.\n\n## Outputs for Interproscan\nThe output of this tool is both a tabular file and an XML file. Both contain the same information, but the tabular one is more readable for a Human: each line represents a gene from our annotation, with the different domains and motifs that were found by InterProScan.\n\nEach line correspond to a motif found in one of the annotated proteins. The most interesting columns are:\n- Column 1: the protein identifier\n- Column 5: the identifier of the signature that was found in the protein sequence\n- Column 4: the databank where this signature comes from (InterProScan regroups several motifs databanks)\n- Column 6: the human readable description of the motif\n- Columns 7 and 8: the position where the motif was found\n- Column 9: a score for the match (if available)\n- Column 12 and 13: identifier of the signature integrated in InterPro (if available). Have a look an example webpage for IPR036859 on InterPro.\n- The following columns contains various identifiers that were assigned to the protein based on the match with the signature (Gene ontology term, Reactome, \u2026)\n",
        "updated": "2024-12-04T00:00:00",
        "categories": [],
        "collections": [
            "Genome Annotation"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/lncRNAs-annotation/main",
        "iwcID": "lncrnas-annotation-main",
        "readme": "# lncRNAs annotation workflow\n\nThis workflow uses the FEELnc tool to annotate long non-coding RNAs. Before annotating these long non-coding RNAs, StringTie will be used to assemble the RNA-seq alignments into potential trancriptions. The gffread tool provides a genome annotation file in GTF format.\n\nFor future analyses, it would be interesting to use an updated annotation containing messenger RNA and long non-coding RNA. The concatenante tool merges the reference annotation with the long non-coding RNA annotation obtained with FEELnc.\n\nThis workflow is taken from the tutorial \u201cLong non-coding RNAs (lncRNAs) annotation with FEELnc\u201d on the GTN.\n\n## Workflows steps\n- Transcript Assembly with StringTie: RNA-seq alignments are assembled into potential transcripts to provide a comprehensive view of expressed regions.\n- Genome Annotation Conversion with GFFRead: Genome annotations are converted into a standardized format (GTF) to ensure compatibility with downstream tools.\n- lncRNA Annotation with FEELnc: The FEELnc pipeline identifies and classifies long non-coding RNAs (lncRNAs) through three main steps:\n    - Filter: Removes unwanted transcripts and those overlapping reference exons.\n    - Codpot: Evaluates coding potential to differentiate lncRNAs from coding RNAs.\n    - Classifier: Assigns lncRNAs to categories based on their genomic location and transcriptional direction.\n- Annotation Merging with Concatenate: The lncRNA annotation is merged with the reference annotation to create a unified genome annotation containing both mRNAs and lncRNAs.\n\n## Input data\nThe following input files are required for the workflow:\n- RNA-seq alignments (BAM format): Required by StringTie for transcript assembly.\n- Genome annotation (GFF3 format): Used by StringTie and GFFRead for processing.\n- Genome sequence (FASTA format): Required by FEELnc for lncRNA identification.\n- Reference annotation (GTF format): Provided by GFFRead for FEELnc analysis.\n\n## Output data\nThe workflow produces the following outputs:\n- Transcript annotation (GTF format): Generated by StringTie, containing assembled transcripts from RNA-seq data.\n- Converted genome annotation (GTF format): Produced by GFFRead, used as input for FEELnc.\n- lncRNA annotation (GTF format): Generated by FEELnc, containing identified lncRNAs.\n- mRNA annotation (GTF format): Produced by FEELnc for downstream use.\n- lncRNA classification table: Produced by FEELnc, detailing genomic relationships of lncRNAs.\n- Comprehensive genome annotation (GTF format): Generated by Concatenate, combining mRNA and lncRNA annotations.\n",
        "updated": "2025-03-05T00:00:00",
        "categories": [],
        "collections": [
            "Genome Annotation"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/dada2/main",
        "iwcID": "dada2-main",
        "readme": "# Dada2: amplicon analysis for paired end data\n\n## Inputs dataset\n\n- `Paired input data` paired input collection in FASTQ format\n\n## Inputs values\n\n- `Read length forward/reverse reads` length of the forward/reverse reads to which they should be truncated in the filter and trim step\n- `Pool samples` pooling may increase sensitivity\n- `Reference database` that should be used for taxonomic assignment\n\n## Processing\n\nThe workflow follows the steps described in the [dada2 tutorial](https://benjjneb.github.io/dada2/tutorial.html).\n\nAs a first step the input collection is sorted. This is important because the dada2 step outputs\na collection in sorted order. If the input collection would not be sorted then the mergePairs step\nsamples would be mixed up.\n\n- `FilterAndTrim` Quality control by filtering and trimming reads\n- `QualityProfile` is called before and after the FilterAndTrim step\n- `Unzip Collection` separates forward and reverse reads (the next steps are evaluated separately on forward and reverse reads)\n- `learnErrors` learn error rates\n- `dada` filter noisy reads\n- `mergePairs` merge forward and reverse reads\n- `makeSequenceTable` create the sequence table\n- `removeBimeraDenovo` remove chimeric sequencs\n- `assignTaxonomy` assign taxonomic information from a reference data base\n\n## TODO\n\nSome possibilities to extend/improve the workflow\n\n- output BIOM\n- use ASV1, ... in sequence table and taxonomy output, and output additional fasta\n- allow to use custom taxonomy / make it optional\n",
        "updated": "2025-02-17T00:00:00",
        "categories": [],
        "collections": [
            "Metabarcoding"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/mgnify-amplicon-pipeline-v5-its/main",
        "iwcID": "mgnify-amplicon-pipeline-v5-its-main",
        "readme": "# MGnify's amplicon (v5.0) ITS subworkflow\n\nClassification and visualization of rRNA sequences based on ITS.\n\n## Input datasets\n\n- **Processed sequences** Post quality control\n- **Clan information file** This file lists which models belong to the same clan `wget ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/pipeline-5.0/ref-dbs/rfam_models/ribosomal_models/ribo.claninfo`\n- **Covariance models** `wget ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/pipeline-5.0/ref-dbs/rfam_models/ribosomal_models/ribo.cm`\n\n## Processing\n\nThis subworkflow executes the following classification and visualization steps:\n\n- **bedtools MaskFastaBed**  is used to mask specific regions in a FASTA file based on provided genomic intervals\n- **MAPseq** Sequence read classification\n- **biom-convert** Converts OTU tables to HDF5 and JSON formats\n- **Krona** Generates pie charts out of the OTU tables",
        "updated": "2025-03-10T00:00:00",
        "categories": [],
        "collections": [
            "Microbiome"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/mapseq-to-ampvis2/main",
        "iwcID": "mapseq-to-ampvis2-main",
        "readme": "# MAPseq to ampvis workflow\n\nThe MAPseq to Ampvis workflow processes MAPseq OTU tables and associated metadata for analysis in Ampvis2. This workflow involves reformatting MAPseq output datasets to produce structured output files suitable for Ampvis2.\n\n## Input datasets\n\n- **MAPseq OTU tables** These tables, generated by MAPseq, include sequence counts and taxonomic classifications.\n- **Metadata** Contextual information associated with the data from the OTU tables.\n\n## Outputs\n\n- **otu_table** A newly formatted OTU table structured for Ampvis2.\n- **tax_table** A taxonomy table containing the taxonomic classifications from the OTU tables.\n- **ampvis2** An R object compatible with the Ampvis2 tool for further analysis.\n- **metadata_list_out**  A structured list of metadata used in the analysis.\n- **taxonomy_list_out** A processed taxonomy list ready for integration with Ampvis2.",
        "updated": "2025-03-10T00:00:00",
        "categories": [],
        "collections": []
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/mgnify-amplicon-pipeline-v5-complete/main",
        "iwcID": "mgnify-amplicon-pipeline-v5-complete-main",
        "readme": "# MGnify's amplicon pipeline (v5.0)\n\n## Input datasets\n\n- **SRA accession list** a File listing the SRA accession IDs\n- **Clan information file** This file lists which models belong to the same clan `wget ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/pipeline-5.0/ref-dbs/rfam_models/ribosomal_models/ribo.claninfo`\n- **Covariance models** `wget ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/pipeline-5.0/ref-dbs/rfam_models/ribosomal_models/ribo.cm`\n\n## Processing\n\nThis pipeline executes the following subworkflows:\n\n- **Quality control - single-end**\n- **Quality control - paired-end**\n- **rRNA-prediction** Classification and visualization of rRNA sequences based on SSUs and LSUs\n- **ITS** Classification and visualization of rRNA sequences based on ITS\n- **Summary tables** Generates taxonomic abundance summary tables\n",
        "updated": "2025-05-05T00:00:00",
        "categories": [],
        "collections": [
            "Microbiome"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/mgnify-amplicon-pipeline-v5-quality-control-paired-end/main",
        "iwcID": "mgnify-amplicon-pipeline-v5-quality-control-paired-end-main",
        "readme": "# MGnify's amplicon (v5.0) quality control subworkflow for paired end data\n\n## Input datasets\n\n- **Paired-end reads** a paired list collection containing the forward and reverse reads.\n\n## Input values\n\n- **fastp - Enable base correction** Enable base correction in overlapped regions, (Default = No)\n- **fastp - Qualified quality phred** The quality value that a base is qualified, (Default = 20)\n- **fastp - Unqualified percent limit** How many percents of bases are allowed to be unqualified, (Default = 20)\n- **fastp - Length required** Reads shorter than this value will be discarded, (Default = 70)\n- **Trimmomatic - SLIDINGWINDOW - Number of bases to average across** (Default = 4)\n- **Trimmomatic - SLIDINGWINDOW - Average quality required** (Default = 15)\n- **Trimmomatic - LEADING** Minimum quality required to keep a base, (Default = 3)\n- **Trimmomatic - TRAILING** Minimum quality required to keep a base, (Default = 3)\n- **Trimmomatic - MINLEN** Minimum length of reads to be kept, (Default = 100)\n- **Trimmomatic - Quality score encoding** The phred+64 encoding works the same as the phred+33 encoding, except you add 64 to the phred score to determine the ascii code of the quality character, (Default = Phred33)\n- **Length filtering - Minimum size** Minimum sequence length, (Default = 100)\n- **Ambiguity filtering - Maximal N percentage threshold to conserve sequences** Maximal N percentage threshold to conserve sequences, (Default = 10)\n\n## Processing\n\nThis subworkflow executes the following quality control steps:\n\n- **fastp** Quality and length filtering\n- **SeqPrep** Merges overlapping reads into a single longer reads using SeqPrep\n- **Trimmomatic** Filtering and trimming reads using trimmomatic\n- **Filter FASTQ** Filter reads shorter than 100 bp\n- **PRINSEQ** Ambiguity filtering\n",
        "updated": "2025-05-05T00:00:00",
        "categories": [],
        "collections": [
            "Microbiome"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/mgnify-amplicon-pipeline-v5-quality-control-single-end/main",
        "iwcID": "mgnify-amplicon-pipeline-v5-quality-control-single-end-main",
        "readme": "# MGnify's amplicon (v5.0) quality control subworkflow for single end data\n\n## Input datasets\n\n- **Single-end reada** A collection containing single-end reads.\n\n## Input values\n\n- **Trimmomatic - SLIDING WINDOW - Average quality required** (Default = 15)\n- **Trimmomatic - LEADING** Minimum quality required to keep a base, (Default = 3)\n- **Trimmomatic - TRAILING** Minimum quality required to keep a base, (Default = 3)\n- **Trimmomatic - SLIDING WINDOW - Number of bases to average across** (Default = 4)\n- **Trimmomatic - MINLEN** Minimum length of reads to be kept, (Default = 100)\n- **Trimmomatic - Quality score encoding** The phred+64 encoding works the same as the phred+33 encoding, except you add 64 to the phred score to determine the ascii code of the quality character, (Default = Phred33)\n- **Length filtering - Minimum size** Minimum sequence length, (Default = 100)\n- **Ambiguity filtering - Maximal N percentage threshold to conserve sequences** Maximal N percentage threshold to conserve sequences, (Default = 10)\n\n## Processing\n\nThis subworkflow executes the following quality control steps:\n\n- **Trimmomatic** Filtering and trimming reads using trimmomatic\n- **Filter FASTQ** Filter reads shorter than 100 bp\n- **PRINSEQ** Ambiguity filtering",
        "updated": "2025-03-10T00:00:00",
        "categories": [],
        "collections": [
            "Microbiome"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/mgnify-amplicon-pipeline-v5-rrna-prediction/main",
        "iwcID": "mgnify-amplicon-pipeline-v5-rrna-prediction-main",
        "readme": "# MGnify's amplicon (v5.0) rRNA-prediction subworkflow\n\nClassification and visualization of rRNA sequences based on SSUs and LSUs.\n\n## Input datasets\n\n- **Processed sequences** Post quality control\n- **Clan information file** This file lists which models belong to the same clan `wget ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/pipeline-5.0/ref-dbs/rfam_models/ribosomal_models/ribo.claninfo`\n- **Covariance models** `wget ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/pipeline-5.0/ref-dbs/rfam_models/ribosomal_models/ribo.cm`\n\n## Processing\n\nThis subworkflow executes the following classification and visualization steps:\n\n- **cmsearch** Searches covariance models against a sequence database\n- **CMsearch-deoverlap** Removes lower scoring overlaps from cmsearch output files\n- **bedtools getfasta**  Extracts sequences from a FASTA file\n- **MAPseq** Sequence read classification\n- **biom-convert** Converts OTU tables to HDF5 and JSON formats\n- **Krona** Generates pie charts out of the OTU tables",
        "updated": "2025-03-10T00:00:00",
        "categories": [],
        "collections": [
            "Microbiome"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/taxonomic-rank-abundance-summary-table/main",
        "iwcID": "taxonomic-rank-abundance-summary-table-main",
        "readme": "# Taxonomic rank abundance summary table\n\nThis subworkflow generates taxonomic abundance summary tables for a user-chosen taxonomic rank.\n\n## Input datasets\n\n- **OTU table collection** containing the taxonomic abundance tables of all datasets.\n\n## Outputs\n\n- Specified taxonomic level abundance summary",
        "updated": "2025-03-10T00:00:00",
        "categories": [],
        "collections": [
            "Microbiome"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/mgnify-amplicon-taxonomic-summary-tables/main",
        "iwcID": "mgnify-amplicon-taxonomic-summary-tables-main",
        "readme": "# MGnify's summary tables (v5.0)\n\nThis subworkflow generates taxonomic abundance summary tables.\n\n## Input datasets\n\n- **OTU table collection** containing the OTU tables of all datasets.\n\n## Outputs\n\n- Phylum level abundance summary table\n- Taxonomic abundance summary table consisting of all levels.",
        "updated": "2025-03-10T00:00:00",
        "categories": [],
        "collections": [
            "Microbiome"
        ]
    },
    {
        "name": "QIIME2-III-V-Phylogeny-Rarefaction-Taxonomic-Analysis",
        "trsID": "#workflow/github.com/iwc-workflows/qiime2-III-VI-downsteam/QIIME2-III-V-Phylogeny-Rarefaction-Taxonomic-Analysis",
        "iwcID": "qiime2-iii-vi-downsteam-qiime2-iii-v-phylogeny-rarefaction-taxonomic-analysis",
        "readme": "# QIIME2 workflows\n\n## Available workflows\n\n- III-V Downstream analyses: III) reconstruct a taxonomy for diversity analysis, IV) rarefaction analysis, V) taxonomic analysis.\n- VI: Computation of diversity metrics and estimations\n\nAnalogous to the procedures described in the Parkinson\u2019s Mouse Tutorial: https://docs.qiime2.org/2024.5/tutorials/pd-mice/\n\n## Inputs\n\nThe two workflows have two inputs in common \n\n- Feature table: Count data\n- Metadata: Metadata table\n\nand the following extra inputs\n\nIII-V\n\n- Representative sequences: Representative (ASV) sequences\n- Minimum depth: Lower limit of the sampling depth for the alpha rarefaction analysis\n- Maximum depth: Upper limit of the sampling depth for the alpha rarefaction analysis\n- SEPP fragment insertion reference: used for the reconstruction of the phylogenetic tree\n- Taxonomic classifier: The classifier to assign taxonomic information to the ASVs\n\nVI:\n\nSampling depth: For the metric calculation (should be based on the rarefaction analysis done in IV)\nTarget metadata parameter: that should be used for beta diversity calculations\nRooted Tree: for instance the tree computed in III\n\n## Processing\n\nIII-V\n\n- Phylogenetic tree generation using `qiime2 fragment-insertion sepp`\n- Alpha rarefaction analysis using `qiime2 diversity alpha-rarefaction`\n- Taxonomic classification using `qiime2 feature-classifier classify-sklearn` and compute barplot and tabular output\n\nVI: \n\n- compute alpha and beta diversity metrics using `qiime2 diversity core-metrics-phylogenetic`\n- organize these metrics in 4 collections:\n  1. Distance matrix collection (weighted and unweighted unifrac, jaccard and bray curtis)\n  2. PCoA collection (same as the distance matrices)\n  3. Emperor plot collection (same as the distance matrices)\n  4. Richness and evenness collection (rarefied table, faith pd vector observed features vector, shannon vector, evenness vector)\n- get visualization for alpha diversity:\n  - Pielou's eveness\n  - Observed features\n  - Shannons diversity index\n- get visualization for beta diversity\n  - Jaccard distance matrix\n  - Bray curtis distance matrix\n  - Unifrac distance metrix \n\n## Outputs\n\nIII-V:\n\n- Phylogenetic tree\n- Rarefaction curve\n- Taxonomic classification (as qza, barplot and table)\n\nVI:\n\n- Four collections containing: distance matrix, PCoA, Emperor plots, Richness and evenness\n- Visualization for alpha diversity: Pielou's eveness, Observed features, Shannons diversity index\n- Visualization for beta diversity: Jaccard, Bray curtis, Unifrac",
        "updated": "2024-11-04T00:00:00",
        "categories": [],
        "collections": [
            "Metabarcoding"
        ]
    },
    {
        "name": "QIIME2-VI-diversity-metrics-and-estimations",
        "trsID": "#workflow/github.com/iwc-workflows/qiime2-III-VI-downsteam/QIIME2-VI-diversity-metrics-and-estimations",
        "iwcID": "qiime2-iii-vi-downsteam-qiime2-vi-diversity-metrics-and-estimations",
        "readme": "# QIIME2 workflows\n\n## Available workflows\n\n- III-V Downstream analyses: III) reconstruct a taxonomy for diversity analysis, IV) rarefaction analysis, V) taxonomic analysis.\n- VI: Computation of diversity metrics and estimations\n\nAnalogous to the procedures described in the Parkinson\u2019s Mouse Tutorial: https://docs.qiime2.org/2024.5/tutorials/pd-mice/\n\n## Inputs\n\nThe two workflows have two inputs in common \n\n- Feature table: Count data\n- Metadata: Metadata table\n\nand the following extra inputs\n\nIII-V\n\n- Representative sequences: Representative (ASV) sequences\n- Minimum depth: Lower limit of the sampling depth for the alpha rarefaction analysis\n- Maximum depth: Upper limit of the sampling depth for the alpha rarefaction analysis\n- SEPP fragment insertion reference: used for the reconstruction of the phylogenetic tree\n- Taxonomic classifier: The classifier to assign taxonomic information to the ASVs\n\nVI:\n\nSampling depth: For the metric calculation (should be based on the rarefaction analysis done in IV)\nTarget metadata parameter: that should be used for beta diversity calculations\nRooted Tree: for instance the tree computed in III\n\n## Processing\n\nIII-V\n\n- Phylogenetic tree generation using `qiime2 fragment-insertion sepp`\n- Alpha rarefaction analysis using `qiime2 diversity alpha-rarefaction`\n- Taxonomic classification using `qiime2 feature-classifier classify-sklearn` and compute barplot and tabular output\n\nVI: \n\n- compute alpha and beta diversity metrics using `qiime2 diversity core-metrics-phylogenetic`\n- organize these metrics in 4 collections:\n  1. Distance matrix collection (weighted and unweighted unifrac, jaccard and bray curtis)\n  2. PCoA collection (same as the distance matrices)\n  3. Emperor plot collection (same as the distance matrices)\n  4. Richness and evenness collection (rarefied table, faith pd vector observed features vector, shannon vector, evenness vector)\n- get visualization for alpha diversity:\n  - Pielou's eveness\n  - Observed features\n  - Shannons diversity index\n- get visualization for beta diversity\n  - Jaccard distance matrix\n  - Bray curtis distance matrix\n  - Unifrac distance metrix \n\n## Outputs\n\nIII-V:\n\n- Phylogenetic tree\n- Rarefaction curve\n- Taxonomic classification (as qza, barplot and table)\n\nVI:\n\n- Four collections containing: distance matrix, PCoA, Emperor plots, Richness and evenness\n- Visualization for alpha diversity: Pielou's eveness, Observed features, Shannons diversity index\n- Visualization for beta diversity: Jaccard, Bray curtis, Unifrac",
        "updated": "2024-11-04T00:00:00",
        "categories": [],
        "collections": [
            "Metabarcoding"
        ]
    },
    {
        "name": "IIa-denoising-se",
        "trsID": "#workflow/github.com/iwc-workflows/qiime2-II-denoising/IIa-denoising-se",
        "iwcID": "qiime2-ii-denoising-iia-denoising-se",
        "readme": "# QIIME2 workflows\n\n## Available workflows\n\nDenoising (using `qiime2`'s `dada2` integration for paired / single end data.\n\n## Inputs\n\n- Demultiplexed sequences as a qiime2 aertifact file (`qza`) containing the sequence information.\n- Metadata table (`tabular`)\n- Truncation length\n- Trimming length (optional)\n\nFor the paired end workflow the truncation and trimming length for the reverse reads can / has to be given.\n\n\n## Processing\n\n- Denoising with `qiime2 dada2 denoise-single`/`paired`\n- For each of the three outputs (see below) another tool is started to prepare a corresponding qzv file\n  - representative sequences `qiime2 feature-table tabulate-seqs `\n  - denoising statistics `qiime2 metadata tabulate`\n  - summary of the feature table\n\n## Outputs\n\n  - representative sequences \n  - denoising statistics \n  - summary of the feature table (how many sequences are lost in the corresponding steps)\n",
        "updated": "2024-11-04T00:00:00",
        "categories": [],
        "collections": [
            "Metabarcoding"
        ]
    },
    {
        "name": "IIb-denoising-pe",
        "trsID": "#workflow/github.com/iwc-workflows/qiime2-II-denoising/IIb-denoising-pe",
        "iwcID": "qiime2-ii-denoising-iib-denoising-pe",
        "readme": "# QIIME2 workflows\n\n## Available workflows\n\nDenoising (using `qiime2`'s `dada2` integration for paired / single end data.\n\n## Inputs\n\n- Demultiplexed sequences as a qiime2 aertifact file (`qza`) containing the sequence information.\n- Metadata table (`tabular`)\n- Truncation length\n- Trimming length (optional)\n\nFor the paired end workflow the truncation and trimming length for the reverse reads can / has to be given.\n\n\n## Processing\n\n- Denoising with `qiime2 dada2 denoise-single`/`paired`\n- For each of the three outputs (see below) another tool is started to prepare a corresponding qzv file\n  - representative sequences `qiime2 feature-table tabulate-seqs `\n  - denoising statistics `qiime2 metadata tabulate`\n  - summary of the feature table\n\n## Outputs\n\n  - representative sequences \n  - denoising statistics \n  - summary of the feature table (how many sequences are lost in the corresponding steps)\n",
        "updated": "2024-11-04T00:00:00",
        "categories": [],
        "collections": [
            "Metabarcoding"
        ]
    },
    {
        "name": "Ia-import-multiplexed-se",
        "trsID": "#workflow/github.com/iwc-workflows/qiime2-I-import/Ia-import-multiplexed-se",
        "iwcID": "qiime2-i-import-ia-import-multiplexed-se",
        "readme": "# QIIME2 import workflows\n\n\n## Available workflows\n\nImport of fastqsanger.gz data into QIIME artifact files.\n\nAvailable for:\n\n- paired / single end data\n- demultiplexed / multiplexed data (the former according to the EMP protocol)\n\nFor data that is multiplexed with another protocol the Galaxy cutadapt tool can be use.\n\n## Inputs\n\n- Single end or paired end reads in fastq format.\n- For demultiplexed data all datasets must be in a single (flat) collection\n  (also paired data).\n\n### Demultiplexed data\n\n- Demultiplexed data must follow the naming scheme `.+_.+_R[12]_001\\.fastq\\.gz`.\n  Any lane information (in the form of `L[0-9][0-9][0-9]_`) in the dataset names\n  is automatically removed.\n\n### Mulmultiplexed data\n\n- Multiplexed data in a single or two fastq.gz dataset(s)\n- Barcodes as fastq.gz file\n- Metadata (a table describing the samples) and a metadata parameter (the name of the column that contains the barcode sequences)\n- A boolean determining if there reverse complement of the barcode sequences shoul dbe used\n\n## Processing\n\nFor demultiplexed data\n\n1. Lane information is removed from the collection identifiers (using `Extract element identifiers`, `Regex Find And Replace` and `Relabel identifiers`)\n2. Import of sequence data using `qiime2 tools import` with `Casava One Eight Laneless Per Sample Directory Format`\n3. Prepare visualisation dataset with `qiime2 demux summarize`\n\nFor multiplexed data\n\n1. Import sequences and metadata with `qiime2 tools import` as `EMP Paired End Directory Format` and `Immutable Metadata Format`, resp.\n2. Demultiplex the sequences with `qiime2 demux emp-paired`/`paired` (using sequences and metadata information)\n3. Prepare visualisation dataset with `qiime2 demux summarize`\n\n\n## Outputs\n\n- Sequence data in `qza` format\n- A corresponding qiime visualization file in `qzv` format\n\n## TODOs\n\n- The import workflows for multiplexed data currently first convert the metadata into qza and require the user to enter a column as free text. If Galaxy allows for data-column workflow parameters this step can be removed.",
        "updated": "2024-11-04T00:00:00",
        "categories": [],
        "collections": [
            "Metabarcoding"
        ]
    },
    {
        "name": "Ib-import-multiplexed-pe",
        "trsID": "#workflow/github.com/iwc-workflows/qiime2-I-import/Ib-import-multiplexed-pe",
        "iwcID": "qiime2-i-import-ib-import-multiplexed-pe",
        "readme": "# QIIME2 import workflows\n\n\n## Available workflows\n\nImport of fastqsanger.gz data into QIIME artifact files.\n\nAvailable for:\n\n- paired / single end data\n- demultiplexed / multiplexed data (the former according to the EMP protocol)\n\nFor data that is multiplexed with another protocol the Galaxy cutadapt tool can be use.\n\n## Inputs\n\n- Single end or paired end reads in fastq format.\n- For demultiplexed data all datasets must be in a single (flat) collection\n  (also paired data).\n\n### Demultiplexed data\n\n- Demultiplexed data must follow the naming scheme `.+_.+_R[12]_001\\.fastq\\.gz`.\n  Any lane information (in the form of `L[0-9][0-9][0-9]_`) in the dataset names\n  is automatically removed.\n\n### Mulmultiplexed data\n\n- Multiplexed data in a single or two fastq.gz dataset(s)\n- Barcodes as fastq.gz file\n- Metadata (a table describing the samples) and a metadata parameter (the name of the column that contains the barcode sequences)\n- A boolean determining if there reverse complement of the barcode sequences shoul dbe used\n\n## Processing\n\nFor demultiplexed data\n\n1. Lane information is removed from the collection identifiers (using `Extract element identifiers`, `Regex Find And Replace` and `Relabel identifiers`)\n2. Import of sequence data using `qiime2 tools import` with `Casava One Eight Laneless Per Sample Directory Format`\n3. Prepare visualisation dataset with `qiime2 demux summarize`\n\nFor multiplexed data\n\n1. Import sequences and metadata with `qiime2 tools import` as `EMP Paired End Directory Format` and `Immutable Metadata Format`, resp.\n2. Demultiplex the sequences with `qiime2 demux emp-paired`/`paired` (using sequences and metadata information)\n3. Prepare visualisation dataset with `qiime2 demux summarize`\n\n\n## Outputs\n\n- Sequence data in `qza` format\n- A corresponding qiime visualization file in `qzv` format\n\n## TODOs\n\n- The import workflows for multiplexed data currently first convert the metadata into qza and require the user to enter a column as free text. If Galaxy allows for data-column workflow parameters this step can be removed.",
        "updated": "2024-11-04T00:00:00",
        "categories": [],
        "collections": [
            "Metabarcoding"
        ]
    },
    {
        "name": "Ic-import-demultiplexed-se",
        "trsID": "#workflow/github.com/iwc-workflows/qiime2-I-import/Ic-import-demultiplexed-se",
        "iwcID": "qiime2-i-import-ic-import-demultiplexed-se",
        "readme": "# QIIME2 import workflows\n\n\n## Available workflows\n\nImport of fastqsanger.gz data into QIIME artifact files.\n\nAvailable for:\n\n- paired / single end data\n- demultiplexed / multiplexed data (the former according to the EMP protocol)\n\nFor data that is multiplexed with another protocol the Galaxy cutadapt tool can be use.\n\n## Inputs\n\n- Single end or paired end reads in fastq format.\n- For demultiplexed data all datasets must be in a single (flat) collection\n  (also paired data).\n\n### Demultiplexed data\n\n- Demultiplexed data must follow the naming scheme `.+_.+_R[12]_001\\.fastq\\.gz`.\n  Any lane information (in the form of `L[0-9][0-9][0-9]_`) in the dataset names\n  is automatically removed.\n\n### Mulmultiplexed data\n\n- Multiplexed data in a single or two fastq.gz dataset(s)\n- Barcodes as fastq.gz file\n- Metadata (a table describing the samples) and a metadata parameter (the name of the column that contains the barcode sequences)\n- A boolean determining if there reverse complement of the barcode sequences shoul dbe used\n\n## Processing\n\nFor demultiplexed data\n\n1. Lane information is removed from the collection identifiers (using `Extract element identifiers`, `Regex Find And Replace` and `Relabel identifiers`)\n2. Import of sequence data using `qiime2 tools import` with `Casava One Eight Laneless Per Sample Directory Format`\n3. Prepare visualisation dataset with `qiime2 demux summarize`\n\nFor multiplexed data\n\n1. Import sequences and metadata with `qiime2 tools import` as `EMP Paired End Directory Format` and `Immutable Metadata Format`, resp.\n2. Demultiplex the sequences with `qiime2 demux emp-paired`/`paired` (using sequences and metadata information)\n3. Prepare visualisation dataset with `qiime2 demux summarize`\n\n\n## Outputs\n\n- Sequence data in `qza` format\n- A corresponding qiime visualization file in `qzv` format\n\n## TODOs\n\n- The import workflows for multiplexed data currently first convert the metadata into qza and require the user to enter a column as free text. If Galaxy allows for data-column workflow parameters this step can be removed.",
        "updated": "2024-11-04T00:00:00",
        "categories": [],
        "collections": [
            "Metabarcoding"
        ]
    },
    {
        "name": "Id-import-demultiplexed-pe",
        "trsID": "#workflow/github.com/iwc-workflows/qiime2-I-import/Id-import-demultiplexed-pe",
        "iwcID": "qiime2-i-import-id-import-demultiplexed-pe",
        "readme": "# QIIME2 import workflows\n\n\n## Available workflows\n\nImport of fastqsanger.gz data into QIIME artifact files.\n\nAvailable for:\n\n- paired / single end data\n- demultiplexed / multiplexed data (the former according to the EMP protocol)\n\nFor data that is multiplexed with another protocol the Galaxy cutadapt tool can be use.\n\n## Inputs\n\n- Single end or paired end reads in fastq format.\n- For demultiplexed data all datasets must be in a single (flat) collection\n  (also paired data).\n\n### Demultiplexed data\n\n- Demultiplexed data must follow the naming scheme `.+_.+_R[12]_001\\.fastq\\.gz`.\n  Any lane information (in the form of `L[0-9][0-9][0-9]_`) in the dataset names\n  is automatically removed.\n\n### Mulmultiplexed data\n\n- Multiplexed data in a single or two fastq.gz dataset(s)\n- Barcodes as fastq.gz file\n- Metadata (a table describing the samples) and a metadata parameter (the name of the column that contains the barcode sequences)\n- A boolean determining if there reverse complement of the barcode sequences shoul dbe used\n\n## Processing\n\nFor demultiplexed data\n\n1. Lane information is removed from the collection identifiers (using `Extract element identifiers`, `Regex Find And Replace` and `Relabel identifiers`)\n2. Import of sequence data using `qiime2 tools import` with `Casava One Eight Laneless Per Sample Directory Format`\n3. Prepare visualisation dataset with `qiime2 demux summarize`\n\nFor multiplexed data\n\n1. Import sequences and metadata with `qiime2 tools import` as `EMP Paired End Directory Format` and `Immutable Metadata Format`, resp.\n2. Demultiplex the sequences with `qiime2 demux emp-paired`/`paired` (using sequences and metadata information)\n3. Prepare visualisation dataset with `qiime2 demux summarize`\n\n\n## Outputs\n\n- Sequence data in `qza` format\n- A corresponding qiime visualization file in `qzv` format\n\n## TODOs\n\n- The import workflows for multiplexed data currently first convert the metadata into qza and require the user to enter a column as free text. If Galaxy allows for data-column workflow parameters this step can be removed.",
        "updated": "2024-11-04T00:00:00",
        "categories": [],
        "collections": [
            "Metabarcoding"
        ]
    },
    {
        "name": "COVID-19-SE-WGS-ILLUMINA",
        "trsID": "#workflow/github.com/iwc-workflows/sars-cov-2-se-illumina-wgs-variant-calling/COVID-19-SE-WGS-ILLUMINA",
        "iwcID": "sars-cov-2-se-illumina-wgs-variant-calling-covid-19-se-wgs-illumina",
        "readme": "COVID-19: variation analysis on WGS SE data\n-------------------------------------------\n\nThis workflows performs single end read mapping with bowtie2 followed by\nsensitive variant calling across a wide range of AFs with lofreq and variant\nannotation with snpEff 4.5covid19.\n",
        "updated": "2025-03-10T00:00:00",
        "categories": [
            "COVID-19"
        ],
        "collections": [
            "SARS-COV-2",
            "Variant Calling"
        ]
    },
    {
        "name": "COVID-19-CONSENSUS-CONSTRUCTION",
        "trsID": "#workflow/github.com/iwc-workflows/sars-cov-2-consensus-from-variation/COVID-19-CONSENSUS-CONSTRUCTION",
        "iwcID": "sars-cov-2-consensus-from-variation-covid-19-consensus-construction",
        "readme": "COVID-19: consensus construction\n--------------------------------\n\nThis workflow aims at generating reliable consensus sequences from variant\ncalls according to transparent criteria that capture at least some of the\ncomplexity of variant calling.\n\nIt takes a collection of VCFs (with DP and DP4 INFO fields) and a collection of\nthe corresponding aligned reads (for the purpose of calculating genome-wide\ncoverage) such as produced by any of the variant calling workflows in\nhttps://github.com/galaxyproject/iwc/tree/main/workflows/sars-cov-2-variant-calling\nand generates a collection of viral consensus sequences and a multisample FASTA\nof all these sequences.\n\nEach consensus sequence is guaranteed to capture all called, filter-passing (as\nper the FILTER column of the VCF input) variants found in the VCF of its sample\nthat reach a user-defined consensus allele frequency threshold.\n\nFilter-failing variants and variants below a second user-defined minimal\nallele frequency threshold will be ignored.\n\nGenomic positions of filter-passing variants with an allele frequency in\nbetween the two thresholds will be hard-masked (with N) in the consensus\nsequence of their sample.\n\nGenomic positions with a coverage (calculated from the read alignments input)\nbelow another user-defined threshold will be hard-masked, too, unless they are\nconsensus variant sites.\n",
        "updated": "2025-03-10T00:00:00",
        "categories": [
            "COVID-19"
        ],
        "collections": [
            "SARS-COV-2"
        ]
    },
    {
        "name": "COVID-19-VARIATION-REPORTING",
        "trsID": "#workflow/github.com/iwc-workflows/sars-cov-2-variation-reporting/COVID-19-VARIATION-REPORTING",
        "iwcID": "sars-cov-2-variation-reporting-covid-19-variation-reporting",
        "readme": "COVID-19: variation analysis reporting\n--------------------------------------\n\nThis workflow takes VCF datasets of variants produced by any of the\n\"*-variant-calling\" workflows in\nhttps://github.com/galaxyproject/iwc/tree/main/workflows/sars-cov-2-variant-calling\nand generates tabular reports of variants by samples and by variant, along with\nan overview plot of variants and their allele-frequencies across all samples.\n",
        "updated": "2024-09-24T00:00:00",
        "categories": [
            "COVID-19"
        ],
        "collections": [
            "SARS-COV-2",
            "Variant Calling"
        ]
    },
    {
        "name": "SARS-COV-2-ILLUMINA-AMPLICON-IVAR-PANGOLIN-NEXTCLADE",
        "trsID": "#workflow/github.com/iwc-workflows/sars-cov-2-pe-illumina-artic-ivar-analysis/SARS-COV-2-ILLUMINA-AMPLICON-IVAR-PANGOLIN-NEXTCLADE",
        "iwcID": "sars-cov-2-pe-illumina-artic-ivar-analysis-sars-cov-2-illumina-amplicon-ivar-pangolin-nextclade",
        "readme": "# COVID-19 sequence analysis on Illumina Amplicon PE data\n\nThis workflow implements an [iVar](https://github.com/andersen-lab/ivar) based analysis similar to\nthe one in [ncov2019-artic-nf](https://github.com/connor-lab/ncov2019-artic-nf), [covid-19-signal](https://github.com/jaleezyy/covid-19-signal/) and the Thiagen [Titan workflow](https://github.com/theiagen/public_health_viral_genomics). These workflows (written in  Nextflow, Snakemake and WDL) are widely in use in [COG UK](https://www.cogconsortium.uk/), [CanCOGeN](https://www.genomecanada.ca/en/cancogen) and some US state public health laboratories.\n\nThis workflow is also the subject of a Galaxy Training Network tutorial (currently a [Work in Progress](https://github.com/galaxyproject/training-material/pull/2633)).\nIt differs from [this workflow](https://github.com/galaxyproject/iwc/tree/main/workflows/sars-cov-2-variant-calling/sars-cov-2-pe-illumina-artic-variant-calling) in\nthat it does not use `lofreq` and is aimed at rapid analysis of majority variants and lineage/clade assignment with `pangolin` and `nextclade`.\n\nTODO:\n\n1. Add support for QC using negative and positive controls\n2. Integrate with phylogeny tools including IQTree and UShER (and possibly more).\n",
        "updated": "2022-11-22T00:00:00",
        "categories": [
            "COVID-19"
        ],
        "collections": [
            "SARS-COV-2",
            "Variant Calling"
        ]
    },
    {
        "name": "COVID-19-PE-WGS-ILLUMINA",
        "trsID": "#workflow/github.com/iwc-workflows/sars-cov-2-pe-illumina-wgs-variant-calling/COVID-19-PE-WGS-ILLUMINA",
        "iwcID": "sars-cov-2-pe-illumina-wgs-variant-calling-covid-19-pe-wgs-illumina",
        "readme": "COVID-19: variation analysis on WGS PE data\n-------------------------------------------\n\nThis workflows performs paired end read mapping with bwa-mem followed by\nsensitive variant calling across a wide range of AFs with lofreq and variant\nannotation with snpEff 4.5covid19.\n",
        "updated": "2023-11-20T00:00:00",
        "categories": [
            "COVID-19"
        ],
        "collections": [
            "SARS-COV-2",
            "Variant Calling"
        ]
    },
    {
        "name": "COVID-19-ARTIC-ONT",
        "trsID": "#workflow/github.com/iwc-workflows/sars-cov-2-ont-artic-variant-calling/COVID-19-ARTIC-ONT",
        "iwcID": "sars-cov-2-ont-artic-variant-calling-covid-19-artic-ont",
        "readme": "COVID-19: variation analysis on ARTIC ONT data\n----------------------------------------------\n\nThis workflow for ONT-sequenced ARTIC data is modeled after the alignment/variant-calling steps of the [ARTIC pipeline](https://artic.readthedocs.io/en/latest/). It performs, essentially, the same steps as that pipeline\u2019s minion command, i.e. read mapping with minimap2 and variant calling with medaka. Like the Illumina ARTIC workflow it uses ivar for primer trimming. Since ONT-sequenced reads have a much higher error rate than Illumina-sequenced reads and are therefor plagued more by false-positive variant calls, this workflow does make no attempt to handle amplicons affected by potential primer-binding site mutations.\n",
        "updated": "2023-11-20T00:00:00",
        "categories": [
            "COVID-19"
        ],
        "collections": [
            "SARS-COV-2",
            "Variant Calling"
        ]
    },
    {
        "name": "COVID-19-PE-ARTIC-ILLUMINA",
        "trsID": "#workflow/github.com/iwc-workflows/sars-cov-2-pe-illumina-artic-variant-calling/COVID-19-PE-ARTIC-ILLUMINA",
        "iwcID": "sars-cov-2-pe-illumina-artic-variant-calling-covid-19-pe-artic-illumina",
        "readme": "COVID-19: variation analysis on ARTIC PE data\n---------------------------------------------\n\nThe workflow for Illumina-sequenced ampliconic data builds on the RNASeq workflow\nfor paired-end data using the same steps for mapping and variant calling, but\nadds extra logic for trimming amplicon primer sequences off reads with the ivar\npackage. In addition, this workflow uses ivar also to identify amplicons\naffected by primer-binding site mutations and, if possible, excludes reads\nderived from such \"tainted\" amplicons when calculating allele-frequencies\nof other variants.\n",
        "updated": "2025-03-17T00:00:00",
        "categories": [
            "COVID-19"
        ],
        "collections": [
            "SARS-COV-2",
            "Variant Calling"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/sra-manifest-to-concatenated-fastqs/main",
        "iwcID": "sra-manifest-to-concatenated-fastqs-main",
        "readme": "# SRA manifest to concatenated fastqs\n\nThis workflow takes as input a SRA manifest from SRA Run Selector (or a tabular with a header line), downloads all sequencing run data from the SRA and arranges it into per-sample fastq or pairs of fastq datasets.\n\nIt will work out the relationship between runs and samples from the user-indicated run and sample columns in the input and will concatenate sequencing run data as needed to obtain per-sample datasets.\n\n## Input dataset\n\n- The workflow needs a single tabular input dataset, which is supposed to list SRA run identifiers in one column and sample names in another, and which needs to have a header line.\n- SRA manifests obtained via the SRA Run Selector and turned into tabular format represent valid input.\n\n## Input values\n\n- Column number with SRA run ID\n\n  For manifests obtained through the SRA Run Selector this is column 1\n\n- Column number with sample names\n\n  The number of the column that should be used to assign sequencing runs to samples\n  The names in the column will also serve as the labels of datasets in the output collection.\n  For manifests obtained through the SRA Run Selector suitable columns might be number 6 (BioSample), 16 (Experiment) or 36 (Sample Name).\n\n## Processing\n\n- The workflow downloads sequencing run data in fastq format with fasterqdump (one job per SRA run ID).\n- Run data gets concatenated if it comes from the same sample.\n\n## Outputs\n\n- There are 2 outputs, one with paired-end datasets, one with single-read datasets.\n\n## Limitations\n\n- Special characters in sample names (anything that is not an English alphabet character, digit, underscore, dash, space, dot or comma (`[a-zA-Z0-9_\\- \\.,]`) will be converted to dashes (`-`).\n",
        "updated": "2024-06-17T00:00:00",
        "categories": [],
        "collections": [
            "Data Fetching"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/parallel-accession-download/main",
        "iwcID": "parallel-accession-download-main",
        "readme": "# Parallel Accession Download\n\nDownloads fastq files for sequencing run accessions provided in a text file\nusing fasterq-dump. Creates one job per listed run accession, and is therefore\nmuch faster and more robust to errors when many accessions need to be\ndownloaded.\n",
        "updated": "2024-05-27T00:00:00",
        "categories": [],
        "collections": [
            "Data Fetching"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/pox-virus-amplicon/main",
        "iwcID": "pox-virus-amplicon-main",
        "readme": "# Pox Virus Illumina Amplicon Workflow for half-genomes sequencing data\n\nThis workflow generates consensus sequences from Illumina PE-sequenced ARTIC data of pox virus samples.\n\nIt requires that all samples have been sequenced in two halves in two separate sequencing runs, and utilizes this property to resolve the inverted terminal repeat (ITR) sequences of pox virus genomes.\n\nThe workflow uses BWA-MEM for mapping the reads from each half-genome sequencing run to a correspondingly masked version of the reference genome, merges the resulting two read mappings, and uses iVar for primer trimming and consensus sequence generation.\n\nConceptually, this workflow builds on https://github.com/iwc-workflows/sars-cov-2-pe-illumina-artic-ivar-analysis and adds the logic for the split genome mapping and merging of the results.",
        "updated": "2025-03-17T00:00:00",
        "categories": [],
        "collections": [
            "Virology"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/influenza-isolates-consensus-and-subtyping/main",
        "iwcID": "influenza-isolates-consensus-and-subtyping-main",
        "readme": "# Subtyping and consensus sequence generation for batches of Influenza A isolates\n\nThis workflow performs subtyping (with respect to the HA and NA genes) and consensus sequence generation for batches of Illumina PE sequenced Influenza isolates. It overcomes Influenza genome variability by compiling the best possible reference for read mapping from a collection of reference sequences for each viral genome segment. The workflow has been tested for Influenza A isolates only but should also work with other types if a suitable collection of per-segment reference sequences (see \"Input datasets\" below) is provided.\n\nIt uses:\n- **VAPOR** for identifying, from a collection of reference sequences of each of the eight viral genome segments, the individual segment sequences that match the sequencing data for each sample most closely\n- **fastp** for sequenced reads pre-processing,\n- **bwa mem** for mapping the preprocessed reads to a reference compiled from the best-matching segment sequences identified by *VAPOR*\n- **ivar consensus** for generating consensus sequences from the mapped reads data\n- **MAFFT** for generating multiple sequence alignments from the consensus sequences of the samples in a batch\n- **Snipit** and **IQ-Tree** for visualizing differences and analyzing relationships between sequences from a batch\n\nThe workflow provides a subtyping report, the consensus sequences arranged by gene segment or by sample, quality control at the level of sequenced reads and of mapping results, and batch-level visualization and phylogenetic insight.\n\n## Input datasets\n\n- **Sequenced paired-end data**: a list of pairs of sequencing datasets, one fw-/rv-reads pair per sequenced isolate\n- **References per segment collection**: this must be provided as a list of FASTA datasets, one for each Influenza genome segment to analyze. Each of these datasets should contain all reference sequences of the corresponding segment that you wish to use in the analysis.\n\n  **NOTE 1**: For subtyping to work correctly *all* FASTA sequence title lines in the input need to follow the scheme (note the use of both `|` and `/`):\n\n  `>[segment]|[influenza type]/[host]/[region]/[internal reference number]/[collection year]|subtype|[accession number]`\n\n  where `[host]` can be omitted for samples obtained from human patients.\n\n  Examples:\n\n  - `>NS|A/California/07/2009|H1N1|NC_026432.1` (for the NS segment sequence of a H1N1 sample obtained from a human patient in California in 2009)\n  - `>NA|A/chicken/Zimbabwe/AI4935/2017|H5N8|MF973227.1` (for the NA segment sequence of a H5N8 sample obtained from chicken in Zimbabwe in 2017)\n\n  **NOTE 2**: Sequences containing the ambiguity symbol N will be ignored by the workflow entirely.\n\n  **NOTE 3**: Datasets in this collection must not use colons (`:`) as part of their names.\n\n  **NOTE 4**: A well-formatted collection of Influenza A reference sequences suitable for most analysis needs is linked from the \"Data resources\" section of the page: https://virology.usegalaxy.eu/published/page?id=a04ab8d6ecb698fa.\n\n## Outputs:\n\n- **successful VAPOR runs - closest references**: a nested collection listing, for each sample and segment, the up to 500 best matching reference sequences according to VAPOR; inspect this collection if you are curious how many good matches to your data there were in the reference collection; useful for debugging, for example, if generated consensus sequences contain unresolved bases (Ns); missing segments or samples in this collection indicate that VAPOR failed to identify any matches for the respective item\n- **Subtyping results**: a table listing one sample and its detected subtype (with respect to the HA and NA segments) per row; missing subtype information for HA, NA or both segments is indicated by H?, N? and H?N?, respectively, in the corresponding sample line.\n- **Hybrid reference genomes used for mapping**: collection of compiled reference genomes (in FASTA format, one per sample) that served as input for bwa-mem; each reference genome consists of the genome segments from the reference collections that best matched the corresponding sample's sequencing data\n- **fastp reports**: QC and read trimming and filtering results from fastp\n- **Final read mapping results**: bwa-mem mapping results in BAM format post-processed with samtools view\n- **QC reports for mapping results**: a collection of reports of QC metrics for the \"Final read mapping results\" generated with Qualimap\n- **Per-sample consensus sequences**: a nested collection of consensus sequences organized first by sample, then by segment\n- **Per-segment consensus sequences with samples combined**: a collection of the same consensus sequences organized by segment; each collection element is a multi-sequence FASTA dataset with the segment-specific sequences of all samples\n- **Multiple sequence alignments per segment**: a collection of multiple sequence alignments generated with MAFFT from each of the \"Per-segment consensus sequences with samples combined\" above; generated only for segments for which at least two samples yielded a consensus sequence\n- **Snipit plots per segment**: a collection of SNP plots across samples generated with Snipit; one plot per segment; generated only for segments for which at least two samples yielded a consensus sequence; the first input sample will be used as the reference in the plot\n- **IQ-Tree per-segment ML tree**, **IQ-Tree per-segment ML distance matrix** and **IQ-Tree per-segment report**: collections of IQ-Tree ML trees, distance matrices and reports; each collection has one element per segment; generated  only for segments for which at least three samples yielded a consensus sequence\n\n## Related training material\n\nhttps://gxy.io/GTN:T00308 guides you through a simplified, manual analysis that still includes the key steps of this workflow.\n\n",
        "updated": "2025-05-02T00:00:00",
        "categories": [],
        "collections": [
            "Virology"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/openms-metaprosip/main",
        "iwcID": "openms-metaprosip-main",
        "readme": "# MetaProSIP: automated inference of elemental fluxes in microbial communities\n\n## Inputs dataset\n\n- `Centroided LC-MS datasets` in mzML (MetaProSIP is mainly tested on data generated by orbitrap instruments)\n- `Fasta Database` in Fasta (aminoacid sequences)\n\n## Inputs values\n\n- `Precursor monoisotopic mass tolerance` (ppm): This value is passed to\n  - MSGFPlusAdapter parameter `Precursor monoisotopic mass tolerance` (-precursor_mass_tolerance)\n  - MetaProSIP parameter `Tolerance in ppm` (-mz_tolerance_ppm)\n- Fixed modifications\n- Variable modifications\n- Labeled element\n\n## Processing\n\n- DecoyDatabase: Add decoy sequences to the Fasta database (for FDR calculation)\n- FeatureFinderCentroided: identify eluting peptides that correspond to isotopologues with natural isotopic distributions \n- MSGFPlusAdapter: identify peptides through peptide fragment fingerprinting (database search)\n- FeatureFinderMultiplex: detect elution profiles of unlabeled peptides\n- PeptideIndexer: annotate protein association to identified peptides\n- FalseDiscoveryRate: Calculate FDR\n- IDMapper: map identified spectra to elution profiles\n- MetaProSIP: calculate the protein-SIP features, to perform functional grouping, and for protein inference\n",
        "updated": "2024-06-17T00:00:00",
        "categories": [],
        "collections": [
            "Proteomics"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/clinicalmp-quantitation/main",
        "iwcID": "clinicalmp-quantitation-main",
        "readme": "# Clinical Metaproteomics 4: Quantitation\n\nQuantitative proteomics is crucial for many important purposes. It allows researchers to measure and compare the levels of proteins or peptides in biological samples. This provides valuable insights into biomarker discovery, comparative analysis, and differential expression studies. Quantitative proteomics also helps in understanding the functional roles of proteins, the composition of protein complexes, and the effects of drugs on protein expression in pharmacological studies. Additionally, it serves as a quality control measure by validating initial protein identifications and providing data normalization for increased accuracy. The quantitative data are essential for hypothesis testing, and systems biology, and have clinical relevance in areas such as disease diagnosis, prognosis, and therapeutic decision-making. In summary, the quantitation workflow in proteomics is essential for understanding the complexities of protein expression and regulation, and it facilitates a wide range of biological and clinical applications.\n\nIn this current workflow, we perform Quantification using the MaxQuant tool. A GTN has been developed for this workflow.\nhttps://training.galaxyproject.org/training-material/topics/proteomics/tutorials/clinical-mp-4-quantitation/tutorial.html\n\n## Inputs dataset\n\n- `RAW MSMS datasets` in RAW dataset collection \n- `Quantitation_Database_for_MaxQuant` in Fasta (protein sequences for database searching)\n- `Experimental-Design Discovery MaxQuant` in Tabular Format \n\n## Inputs values\n\nFor MaxQuant \n- Peptide Length\n- Variable modifications\n- Labeled element\n\n\n## Processing\n\n- extract microbial proteins and peptides using Select and Cut\n- Grouping duplicates using the Group tool\n",
        "updated": "2024-08-14T00:00:00",
        "categories": [],
        "collections": [
            "Proteomics",
            "Metaproteomics"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/clinicalmp-verification/main",
        "iwcID": "clinicalmp-verification-main",
        "readme": "# Clinical Metaproteomics 3: Verification\nThis workflow uses the PepQuery tool to verify peptides discovered with the clinical metaproteomics discovery workflow.\nThe PepQuery tool outputs verified peptides that can be used to generate a verified protein database that can be used for the clinical metaproteomics quantitation workflow.\n\nMore background on this workflow can be found in the [Clinical Metaproteomics 3: Verification tutorial](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/clinical-mp-3-verification/tutorial.html)\n\n## Inputs datasets\n\n### Tabular Input Files\n- SGPS peptide report\n- MaxQuant Peptide report\n- Distinct Peptides for PepQuery\n\n### Search Databases (FASTA)\n- Uniprot HUMAN database\n- cRAP\n\n### MSMS files\nThe tandem MS/MS files can be downloaded via Zenodo. These MS/MS input files are pilot datasets from Papanicolaou test samples from healthy, benign and ovarian cancer patients. \n\n## Input Values\nFor PepQuery:\n- Search Tolerances\n- Digestion Enzyme\n- Peptide Length\n- Modifications\n\n## Processing\nExtract protein sequences for the verified peptides.\n",
        "updated": "2024-12-16T00:00:00",
        "categories": [],
        "collections": [
            "Proteomics",
            "Metaproteomics"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/clinicalmp-discovery/main",
        "iwcID": "clinicalmp-discovery-main",
        "readme": "# Clinical Metaproteomics 2: Discovery\n\nDiscovery in clinical metaproteomics is greatly enhanced by using a well-curated database, particularly one generated with the **MetaNovo tool**. This tool creates a manageable and streamlined database by identifying proteins relevant to the dataset, reducing the complexity of downstream analysis. For optimal results, the MetaNovo-generated database can be merged with reviewed proteins from **Human SwissProt** and known contaminants from the **cRAP (common Repository of Adventitious Proteins)** database, resulting in a compact yet comprehensive database of approximately 21,200 protein sequences. This refined database serves as the foundation for peptide identification, where mass spectrometry (MS) data is matched against the database to identify relevant peptides efficiently and accurately. By reducing redundancy and focusing on clinically relevant sequences, this approach improves the discovery of biomarkers and key protein insights, allowing researchers to extract meaningful biological information with reduced noise and false positives. This streamlined process is particularly valuable in clinical studies, where precision and relevance are critical for advancing diagnostics and therapeutic research.\n\nIn this current workflow, we perform Discovery using the SearchGUI and MaxQuant tools. A GTN has been developed for this workflow.\n[https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/clinical-mp-2-discovery/tutorial.html](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/clinical-mp-2-discovery/tutorial.html)\n\n## Inputs dataset\n\n- `MSMS datasets` in RAW dataset collection\n- `Databases for discovery` in Fasta (protein sequences for database searching)\n- `Experimental-Design Discovery MaxQuant` in Tabular Format \n\n## Inputs values\n\nFor MaxQuant and SearchGUI/PeptideShaker \n- Peptide Length\n- Variable modifications\n- Labeled element\n\n\n## Processing\n\n- extract microbial proteins and peptides using text formating tools\n- Grouping duplicates using the Group tool\n",
        "updated": "2024-11-18T00:00:00",
        "categories": [],
        "collections": [
            "Proteomics",
            "Metaproteomics"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/clinicalmp-data-interpretation/main",
        "iwcID": "clinicalmp-data-interpretation-main",
        "readme": "# Clinical Metaproteomics 5: Data Interpretation\nData interpretation can provide meaningful insights about quantitative proteomic data. Here, MaxQuant data will be interpreted using MSstats by applying a rigorous statistical framework to understand data distribution and variability. Systematic variations are accounted for using subsequent normalization. Users can define the experimental design used by MSstats, such as sample groups and conditions, to perform statistical analysis. The MSstats output provides valuable information about differential protein expression across various conditions, estimates of fold changes, and associated p-values, which aids in the identification of biologically significant proteins. Furthermore, MSstats enables quality control and data visualization, enhancing our ability to draw meaningful conclusions from large-scale proteomic datasets and expands our understanding of complex biological processes.\n\nIn this current workflow, we perform taxonomic and functional annotations using Unipept and statistical analysis using MSstatsTMT. A GTN has been developed for this workflow.\n[https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/clinical-mp-5-data-interpretation/tutorial.html](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/clinical-mp-5-data-interpretation/tutorial.html).\n\n## Input datasets\n\n### Input tabular files\n- `Annotation.tabular`\n- `Comparison_Matrix.tabular`\n- `MaxQuant_Evidence.tabular`\n- `MaxQuant_Protein_Groups.tabular`\n- `Quantified-Peptides.tabular`\n\n## Input Values\n\nFor Unipept:\n- Unipept application: peptinfo: Tryptic peptides and associated EC and GO terms and lowest common ancestor taxonomy\n- Match input peptides by: Match to the full input peptide\n\nFor MSstatsTMT:\n- Generate separate plots for microbial and human proteins\n\n## Processing\n- Perform taxonomic and functional annotations for quantified microbial peptides using Unipept.\n- Select microbial and human protein groups to perform statistical analysis using MSstatsTMT.\n",
        "updated": "2024-11-19T00:00:00",
        "categories": [],
        "collections": [
            "Proteomics",
            "Metaproteomics"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/clinicalmp-database-generation/main",
        "iwcID": "clinicalmp-database-generation-main",
        "readme": "# Clinical Metaproteomics 1: Database Generation\nMetaproteomics involves the large-scale identification and analysis of all proteins expressed by microbiota. However, analyzing clinical samples using metaproteomics is complicated by the presence of abundant human (host) proteins, which can obscure the detection of less abundant microbial proteins.\n\nTo overcome this challenge, we developed a metaproteomics workflow using tandem mass spectrometry (MS/MS) and bioinformatics tools on the Galaxy platform. This workflow enables the characterization of metaproteomes in clinical samples.\n\nThe first step in this workflow is the Database Generation process. The Galaxy-P team has created a workflow that compiles a large database by downloading protein sequences of known disease-causing microorganisms. From this extensive database, a compact, relevant database is then created using the Metanovo tool.\nA GTN has been developed for this workflow. [https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/clinical-mp-1-database-generation/tutorial.html](https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/clinical-mp-1-database-generation/tutorial.html)\n\n## Inputs dataset\n\n### Search Databases (FASTA) from [Zenodo](https://zenodo.org/records/14181725)\n- `HUMAN SwissProt Protein_Database.fasta`\n- `Species UniProt Protein Database FASTA.fasta`\n- `Contaminants (cRAP) Protein Database.fasta`\n\n### MSMS files download from [Zenodo](https://zenodo.org/records/14181725)\n- `PTRC_Skubitz_Plex2_F10_9Aug19_Rage_Rep-19-06-08.mgf`\n- `PTRC_Skubitz_Plex2_F11_9Aug19_Rage_Rep-19-06-08.mgf`\n- `PTRC_Skubitz_Plex2_F13_9Aug19_Rage_Rep-19-06-08.mgf`\n- `PTRC_Skubitz_Plex2_F15_9Aug19_Rage_Rep-19-06-08.mgf`\n\n## Input Values\nFor Metanovo \n- Peptide Length\n- Variable modifications\n- Labeled element\n\n## Processing\n- Merge all the resultant FASTA files\n",
        "updated": "2024-11-18T00:00:00",
        "categories": [],
        "collections": [
            "Proteomics",
            "Metaproteomics"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/gromacs-mmgbsa/main",
        "iwcID": "gromacs-mmgbsa-main",
        "readme": "# GROMACS MMGBSA free energy calculation\n\nPerform an ensemble of MD simulations of a user-specified size using GROMACS,\nand calculate MMGBSA free energies using AmberTools. An ensemble average is\ncalculated and returned to the user as the final input.\n\nThe input protein (PDB) and ligand (SDF) files provided are parameterized by\nthe 'Protein-ligand complex parameterization' subworkflow.\n",
        "updated": "2023-11-27T00:00:00",
        "categories": [
            "Structural Biology"
        ],
        "collections": [
            "Computational Chemistry"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/gromacs-dctmd/main",
        "iwcID": "gromacs-dctmd-main",
        "readme": "# GROMACS dcTMD free energy calculation\n\nPerform an ensemble of targeted MD simulations of a user-specified size using\nthe GROMACS PULL code and calculate dcTMD free energy and friction profiles\nfor the resulting dissocation pathway. Note that pathway separation is not\nperformed by the workflow; the user is responsible for checking the ensemble themselves.\n\nThe input protein (PDB) and ligand (SDF) files provided are parameterized by\nthe 'Protein-ligand complex parameterization' subworkflow.\n\nNote that the workflow uses a MDP file for configuring the TMD simulations; this\nis packaged alongside the workflow as `tmd.mdp`.\n\n## Citations\n* Steffen Wolf and Gerhard Stock (2018), Targeted Molecular Dynamics Calculations of Free Energy Profiles Using a Nonequilibrium Friction Correction, J. Chem. Theory Comput. doi:10.1021/acs.jctc.8b00835\n* Steffen Wolf, Benjamin Lickert, Simon Bray and Gerhard Stock (2020), Multisecond ligand dissociation dynamics from atomistic simulations, Nat. Commun. doi:10.1038/s41467-020-16655-1\n",
        "updated": "2023-11-20T00:00:00",
        "categories": [
            "Structural Biology"
        ],
        "collections": [
            "Computational Chemistry"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/protein-ligand-complex-parameterization/main",
        "iwcID": "protein-ligand-complex-parameterization-main",
        "readme": "# Protein-ligand complex parameterization\n\nParameterizes an input protein (PDB) and ligand (SDF) file prior to molecular\ndynamics simulation with GROMACS.\n\nThis is a simple workflow intended for use as a subworkflow in more complex\nMD workflows. It is used as a subworkflow by the GROMACS MMGBSA and dcTMD\nworkflows. \n",
        "updated": "2023-11-20T00:00:00",
        "categories": [
            "Structural Biology"
        ],
        "collections": [
            "Computational Chemistry"
        ]
    },
    {
        "name": "main",
        "trsID": "#workflow/github.com/iwc-workflows/fragment-based-docking-scoring/main",
        "iwcID": "fragment-based-docking-scoring-main",
        "readme": "# Fragment-based virtual screening with docking and pose scoring\n\nDock a compound library against a target protein with rDock and validate the\nposes generated against a reference fragment using SuCOS to compare the feature\noverlap. Poses are filtered by a user-specified SuCOS threshold.\n\nA list of fragments should be specified which will be used to define the cavity\nfor docking, using the 'Frankenstein ligand' technique. For more details, please\nsee https://www.informaticsmatters.com/blog/2018/11/23/cavities-and-frankenstein-molecules.html\n\nCompounds are split into collections and then recombined to allow the workflow\nto be run in a highly parallelized fashion. To specify the level of\nparallelization, use the 'Collection size' parameter.\n",
        "updated": "2023-11-20T00:00:00",
        "categories": [
            "Structural Biology"
        ],
        "collections": [
            "Computational Chemistry"
        ]
    }
]